{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FAUqEGOtatCT"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.nn import functional as F\n","# import dataset\n","from torch.utils.data import DataLoader, Dataset\n","# import pad sequence\n","import numpy as np\n","import os\n","import numpy as np\n","import librosa\n","from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score\n","from sklearn.model_selection import train_test_split, KFold\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import classification_report\n","\n","avarage = 'micro'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2TO5cCVatCX"},"outputs":[],"source":["class Attention(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, groups=1, reduction=0.0625, kernel_num=4, min_channel=16):\n","        super(Attention, self).__init__()\n","        attention_channel = max(int(in_planes * reduction), min_channel)\n","        self.kernel_size = kernel_size\n","        self.kernel_num = kernel_num\n","        self.temperature = 1.0\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Conv2d(in_planes, attention_channel, 1, bias=False)\n","        self.bn = nn.BatchNorm2d(attention_channel)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.channel_fc = nn.Conv2d(attention_channel, in_planes, 1, bias=True)\n","        self.func_channel = self.get_channel_attention\n","\n","        if in_planes == groups and in_planes == out_planes:  # depth-wise convolution\n","            self.func_filter = self.skip\n","        else:\n","            self.filter_fc = nn.Conv2d(attention_channel, out_planes, 1, bias=True)\n","            self.func_filter = self.get_filter_attention\n","\n","        if kernel_size == 1:  # point-wise convolution\n","            self.func_spatial = self.skip\n","        else:\n","            self.spatial_fc = nn.Conv2d(attention_channel, kernel_size * kernel_size, 1, bias=True)\n","            self.func_spatial = self.get_spatial_attention\n","\n","        if kernel_num == 1:\n","            self.func_kernel = self.skip\n","        else:\n","            self.kernel_fc = nn.Conv2d(attention_channel, kernel_num, 1, bias=True)\n","            self.func_kernel = self.get_kernel_attention\n","\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            if isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def update_temperature(self, temperature):\n","        self.temperature = temperature\n","\n","    @staticmethod\n","    def skip(_):\n","        return 1.0\n","\n","    def get_channel_attention(self, x):\n","        channel_attention = torch.sigmoid(self.channel_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n","        return channel_attention\n","\n","    def get_filter_attention(self, x):\n","        filter_attention = torch.sigmoid(self.filter_fc(x).view(x.size(0), -1, 1, 1) / self.temperature)\n","        return filter_attention\n","\n","    def get_spatial_attention(self, x):\n","        spatial_attention = self.spatial_fc(x).view(x.size(0), 1, 1, 1, self.kernel_size, self.kernel_size)\n","        spatial_attention = torch.sigmoid(spatial_attention / self.temperature)\n","        return spatial_attention\n","\n","    def get_kernel_attention(self, x):\n","        kernel_attention = self.kernel_fc(x).view(x.size(0), -1, 1, 1, 1, 1)\n","        kernel_attention = F.softmax(kernel_attention / self.temperature, dim=1)\n","        return kernel_attention\n","\n","    def forward(self, x):\n","        x = self.avgpool(x)\n","        x = self.fc(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        return self.func_channel(x), self.func_filter(x), self.func_spatial(x), self.func_kernel(x)\n","\n","\n","class ODConv2d(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1,\n","                 reduction=0.0625, kernel_num=4):\n","        super(ODConv2d, self).__init__()\n","        self.in_planes = in_planes\n","        self.out_planes = out_planes\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.dilation = dilation\n","        self.groups = groups\n","        self.kernel_num = kernel_num\n","        self.attention = Attention(in_planes, out_planes, kernel_size, groups=groups,\n","                                   reduction=reduction, kernel_num=kernel_num)\n","        self.weight = nn.Parameter(torch.randn(kernel_num, out_planes, in_planes//groups, kernel_size, kernel_size),\n","                                   requires_grad=True)\n","        self._initialize_weights()\n","\n","        if self.kernel_size == 1 and self.kernel_num == 1:\n","            self._forward_impl = self._forward_impl_pw1x\n","        else:\n","            self._forward_impl = self._forward_impl_common\n","\n","    def _initialize_weights(self):\n","        for i in range(self.kernel_num):\n","            nn.init.kaiming_normal_(self.weight[i], mode='fan_out', nonlinearity='relu')\n","\n","    def update_temperature(self, temperature):\n","        self.attention.update_temperature(temperature)\n","\n","    def _forward_impl_common(self, x):\n","        # Multiplying channel attention (or filter attention) to weights and feature maps are equivalent,\n","        # while we observe that when using the latter method the models will run faster with less gpu memory cost.\n","        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n","        batch_size, in_planes, height, width = x.size()\n","        x = x * channel_attention\n","        x = x.reshape(1, -1, height, width)\n","        aggregate_weight = spatial_attention * kernel_attention * self.weight.unsqueeze(dim=0)\n","        aggregate_weight = torch.sum(aggregate_weight, dim=1).view(\n","            [-1, self.in_planes // self.groups, self.kernel_size, self.kernel_size])\n","        output = F.conv2d(x, weight=aggregate_weight, bias=None, stride=self.stride, padding=self.padding,\n","                          dilation=self.dilation, groups=self.groups * batch_size)\n","        output = output.view(batch_size, self.out_planes, output.size(-2), output.size(-1))\n","        output = output * filter_attention\n","        return output\n","\n","    def _forward_impl_pw1x(self, x):\n","        channel_attention, filter_attention, spatial_attention, kernel_attention = self.attention(x)\n","        x = x * channel_attention\n","        output = F.conv2d(x, weight=self.weight.squeeze(dim=0), bias=None, stride=self.stride, padding=self.padding,\n","                          dilation=self.dilation, groups=self.groups)\n","        output = output * filter_attention\n","        return output\n","\n","    def forward(self, x):\n","        return self._forward_impl(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YcIM0k3atCY"},"outputs":[],"source":["class BasicConv(nn.Module):\n","    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n","        super(BasicConv, self).__init__()\n","        self.out_channels = out_planes\n","        self.conv = ODConv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups)\n","        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n","        self.relu = nn.ReLU() if relu else None\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.bn is not None:\n","            x = self.bn(x)\n","        if self.relu is not None:\n","            x = self.relu(x)\n","        return x\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","class ChannelGate(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n","        super(ChannelGate, self).__init__()\n","        self.gate_channels = gate_channels\n","        self.mlp = nn.Sequential(\n","            Flatten(),\n","            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n","            nn.ReLU(),\n","            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n","            )\n","        self.pool_types = pool_types\n","    def forward(self, x):\n","        channel_att_sum = None\n","        for pool_type in self.pool_types:\n","            if pool_type=='avg':\n","                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( avg_pool )\n","            elif pool_type=='max':\n","                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( max_pool )\n","            elif pool_type=='lp':\n","                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n","                channel_att_raw = self.mlp( lp_pool )\n","            elif pool_type=='lse':\n","                # LSE pool only\n","                lse_pool = logsumexp_2d(x)\n","                channel_att_raw = self.mlp( lse_pool )\n","\n","            if channel_att_sum is None:\n","                channel_att_sum = channel_att_raw\n","            else:\n","                channel_att_sum = channel_att_sum + channel_att_raw\n","\n","        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n","        return x * scale\n","\n","def logsumexp_2d(tensor):\n","    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n","    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n","    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n","    return outputs\n","\n","class ChannelPool(nn.Module):\n","    def forward(self, x):\n","        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n","\n","class SpatialGate(nn.Module):\n","    def __init__(self):\n","        super(SpatialGate, self).__init__()\n","        kernel_size = 7\n","        self.compress = ChannelPool()\n","        self.spatial = ODConv2d(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2)\n","    def forward(self, x):\n","        x_compress = self.compress(x)\n","        x_out = self.spatial(x_compress)\n","        scale = F.sigmoid(x_out) # broadcasting\n","        return x * scale\n","\n","class CBAM(nn.Module):\n","    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n","        super(CBAM, self).__init__()\n","        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n","        self.no_spatial=no_spatial\n","        if not no_spatial:\n","            self.SpatialGate = SpatialGate()\n","    def forward(self, x):\n","        x_out = self.ChannelGate(x)\n","        if not self.no_spatial:\n","            x_out = self.SpatialGate(x_out)\n","        return x_out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgqN_6wfatCZ"},"outputs":[],"source":["class Dual(nn.Module):\n","    def __init__(self):\n","        super(Dual, self).__init__()\n","\n","        # self.feature_extractor1 = nn.Sequential(\n","        #     # 1st Conv Layer + BatchNorm + ReLU + Pooling\n","        #     nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n","        #     nn.BatchNorm1d(64),\n","        #     nn.ReLU(),\n","        #     nn.MaxPool1d(kernel_size=4, stride=4),\n","\n","        #     # 2nd Conv Layer + BatchNorm + ReLU + Pooling\n","        #     nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","        #     nn.BatchNorm1d(128),\n","        #     nn.ReLU(),\n","        #     nn.MaxPool1d(kernel_size=4, stride=4),\n","\n","        #     # 3rd Conv Layer + BatchNorm + ReLU + Pooling\n","        #     nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n","        #     nn.BatchNorm1d(128),\n","        #     nn.ReLU(),\n","        #     nn.MaxPool1d(kernel_size=4, stride=4),\n","\n","        #     # 4th Conv Layer + BatchNorm + ReLU + Pooling\n","        #     nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n","        #     nn.BatchNorm1d(128),\n","        #     nn.ReLU(),\n","        #     nn.MaxPool1d(kernel_size=4, stride=4)\n","        # )\n","\n","        # self.lstm = nn.LSTM(input_size=128, hidden_size=256, batch_first=True)\n","        # # out put lstm (batch_size, seq_len, hidden_size) (batch_size, 1, 256)\n","        # self.fc1 = nn.Linear(256, 5)\n","\n","        self.feature_extractor2 = nn.Sequential(\n","            # 1st Conv Layer + BatchNorm + ReLU + Pooling\n","            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (128, 251) -> (128, 251, 64)\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  # (128, 251, 64) -> (64, 125, 64)\n","\n","            # 2nd Conv Layer + BatchNorm + ReLU + Pooling\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (64, 125, 64) -> (64, 125, 64)\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4)),  # (64, 125, 64) -> (16, 31, 64)\n","\n","            # 3rd Conv Layer + BatchNorm + ReLU + Pooling\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (16, 31, 64) -> (16, 31, 128)\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4)),  # (16, 31, 128) -> (4, 7, 128)\n","\n","            # 4th Conv Layer + BatchNorm + ReLU + Pooling\n","            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),  # (4, 7, 128) -> (4, 7, 128)\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(4, 4), stride=(4, 4))  # (4, 7, 128) -> (1, 1, 128)\n","        )\n","\n","        # Fully Connected Layer\n","        self.fc2 = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(256, 512),  # 1*1*128 -> 256\n","            nn.ReLU(),\n","            nn.Linear(512, 5),  # 256 -> 5\n","        )\n","\n","        # self.truefc = nn.Softmax(dim=1)\n","        self.gru = nn.GRU(input_size=128, hidden_size=256, bidirectional=False, batch_first=True)\n","        self.fc3 = nn.Linear(5, 5)\n","        self.cbam = CBAM(128)\n","\n","    def forward(self, mfcc):\n","        # wave_form = self.feature_extractor1(wave_form)  # Pass through the sequential feature extractor\n","\n","        # # LSTM expects input of shape (batch_size, seq_len, input_size)\n","        # wave_form = wave_form.permute(0, 2, 1)  # Reshape to (batch_size, seq_len, input_size)\n","        # wave_form, _ = self.lstm(wave_form)\n","\n","        # wave_form = wave_form[:, -1, :]\n","\n","        # wave_form = self.fc1(wave_form)\n","\n","        mfcc = self.feature_extractor2(mfcc)\n","        mfcc = self.cbam(mfcc)\n","        mfcc = mfcc.squeeze().unsqueeze(1)\n","\n","        mfcc,_ = self.gru(mfcc)\n","        mfcc = self.fc2(mfcc)\n","        # x = torch.cat((wave_form, mfcc), dim=1)\n","\n","        mfcc = self.fc3(mfcc)\n","\n","        return mfcc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lxCFN8YRatCa"},"outputs":[],"source":["n_mfcc = 128\n","window_size = 2048\n","strides = 512\n","window_size_stft = 1024\n","window = np.hanning(window_size_stft)\n","\n","def load_emodata(link, sr = 16000, duration = 5):\n","    # Load the audio file\n","    wave_form, _ = librosa.load(path=link, sr=sr)\n","    # label = father folder name\n","    labels = os.path.basename(os.path.dirname(link))\n","\n","    if len(wave_form) < sr * duration:\n","        # zero pad the audio if its less than 5 seconds\n","        wave_form = np.pad(wave_form, (0, sr * duration - len(wave_form)), 'symmetric')\n","        mfcc1 = librosa.feature.mfcc(y=wave_form, sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n","        # stft1 = librosa.core.spectrum.stft(wave_form, n_fft=window_size_stft, hop_length=256, window=window)\n","        # spect1 = 2 * np.abs(stft1) / np.sum(window)\n","        return np.array([mfcc1]), np.array([labels])\n","\n","    elif len(wave_form) == sr * duration:\n","        mfcc2 = librosa.feature.mfcc(y=wave_form, sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n","        # stft2 = librosa.core.spectrum.stft(wave_form, n_fft=window_size_stft, hop_length=256, window=window)\n","        # spect2 = 2 * np.abs(stft2) / np.sum(window)\n","        # return the audio as it is\n","        return np.array([mfcc2]), np.array([labels])\n","\n","\n","    else:\n","        wave_segments = []\n","        _labels = [labels] * (len(wave_form) // (sr * duration) + 1)\n","        for i in range(0, len(wave_form), sr * duration):\n","            wave_segments.append(wave_form[i:i + sr * duration])\n","\n","        # If the last segment is less than 5 seconds, then pad it with the last 5 seconds of the second last segment\n","        len_wave_segments_last = len(wave_segments[-1])\n","        padding = sr * duration - len_wave_segments_last\n","        temp = np.append(wave_segments[-2][sr * duration - padding:], wave_segments[-1])\n","        wave_segments[-1] = temp\n","\n","        mfcc_seg = librosa.feature.mfcc(y=np.array(wave_segments), sr=8000, n_mfcc=n_mfcc, n_fft=window_size, hop_length=strides)\n","        # stft_seg = librosa.core.spectrum.stft(wave_segments, n_fft=window_size_stft, hop_length=256, window=window)\n","        # spect_seg = 2 * np.abs(stft_seg) / np.sum(window)\n","\n","        return np.array(mfcc_seg) , np.array(_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7USLCOQUatCa"},"outputs":[],"source":["data_folder = \"../VNEMOS/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAYz1TdvatCa"},"outputs":[],"source":["def load_data(data_folder):\n","    # waves = []\n","    mfccs = []\n","    labels = []\n","    for root, dirs, files in os.walk(data_folder):\n","        for file in files:\n","            if file.endswith(\".wav\"):\n","                link = os.path.join(root, file)\n","                mfcc, label = load_emodata(link)\n","\n","                # waves.extend(wave)\n","                mfccs.extend(mfcc)\n","                labels.extend(label)\n","\n","    return np.array(mfccs), np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fKEDR32atCb"},"outputs":[],"source":["mfccs, labels = load_data(data_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1724873769738,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"4Nq4YNb6atCb","outputId":"459c96c0-5d51-42d6-b33f-660c457be2a1"},"outputs":[{"data":{"text/plain":["(528, 528)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(mfccs), len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9exxxBmPatCc"},"outputs":[],"source":["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pXufmmlatCd"},"outputs":[],"source":["class EmoDataset(Dataset):\n","    def __init__(self, mfccs, labels):\n","\n","        self.mfccs = mfccs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.mfccs)\n","\n","    def __getitem__(self, index):\n","        return self.mfccs[index], self.labels[index]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHKlEstwatCd"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnNwp0bQatCd"},"outputs":[],"source":["EMO_CLASSES = {label: i for i, label in enumerate(np.unique(labels))}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1724873769739,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"U-O3eW7watCd","outputId":"bf6f8503-0fb8-4917-d6ef-3dc8c8d599f2"},"outputs":[{"data":{"text/plain":["{'angry': 0, 'fear': 1, 'happiness': 2, 'neutral': 3, 'sadness': 4}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["EMO_CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1724873769739,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"_uVWo4-_atCd","outputId":"665c9891-5c5f-449d-dd3c-e8a56aa625c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data:  Counter({4: 108, 2: 97, 3: 88, 0: 66, 1: 63})\n","Test data:  Counter({4: 26, 2: 25, 3: 22, 0: 17, 1: 16})\n","Train data:  Counter({4: 107, 2: 97, 3: 88, 0: 66, 1: 64})\n","Test data:  Counter({4: 27, 2: 25, 3: 22, 0: 17, 1: 15})\n","Train data:  Counter({4: 107, 2: 98, 3: 88, 0: 66, 1: 63})\n","Test data:  Counter({4: 27, 2: 24, 3: 22, 0: 17, 1: 16})\n","Train data:  Counter({4: 107, 2: 98, 3: 88, 0: 67, 1: 63})\n","Test data:  Counter({4: 27, 2: 24, 3: 22, 1: 16, 0: 16})\n","Train data:  Counter({4: 107, 2: 98, 3: 88, 0: 67, 1: 63})\n","Test data:  Counter({4: 27, 2: 24, 3: 22, 1: 16, 0: 16})\n"]}],"source":["for idx, (train_idx, test_idx) in enumerate(skf.split(mfccs, labels)):\n","\n","    # wave_train, wave_test = waves[train_idx], waves[test_idx]\n","    mfcc_train, mfcc_test = mfccs[train_idx], mfccs[test_idx]\n","    train_labels, test_labels = labels[train_idx], labels[test_idx]\n","\n","    train_labels = [EMO_CLASSES[label] for label in train_labels]\n","    test_labels = [EMO_CLASSES[label] for label in test_labels]\n","\n","\n","    train_dataset = EmoDataset(mfcc_train, train_labels)\n","    test_dataset = EmoDataset(mfcc_test, test_labels)\n","\n","    print(\"Train data: \", Counter(train_labels))\n","    print(\"Test data: \", Counter(test_labels))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5bOHKixatCd"},"outputs":[],"source":["epochs = 100\n","batch_size = 32\n","learning_rate = 0.001\n","loss_fn = nn.CrossEntropyLoss()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1U0uqadXatCe"},"outputs":[],"source":["EMO_CLASSES = {label: i for i, label in enumerate(set(labels))}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXQzUYlTatCe"},"outputs":[],"source":["acc_kfold = []\n","f1_kfold = []\n","recall_kfold = []\n","precision_kfold = []\n","data_kfold = []\n","labels_preds_kfold = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108981,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"Z4x_I9yfatCe","outputId":"698e45ea-ba92-4db5-cc7e-b9361c7068f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["K-Fold:  0 Epoch:  0 Train loss:  1.5622512783323015\n","K-Fold: 0, Epoch: 0, Accuracy: 0.3867924528301887, Recall: 0.3867924528301887, F1: 0.38679245283018876, Precision: 0.3867924528301887\n","K-Fold:  0 Epoch:  1 Train loss:  1.3775563410350256\n","K-Fold: 0, Epoch: 1, Accuracy: 0.330188679245283, Recall: 0.330188679245283, F1: 0.330188679245283, Precision: 0.330188679245283\n","K-Fold:  0 Epoch:  2 Train loss:  1.2603134725775038\n","K-Fold: 0, Epoch: 2, Accuracy: 0.39622641509433965, Recall: 0.39622641509433965, F1: 0.39622641509433965, Precision: 0.39622641509433965\n","K-Fold:  0 Epoch:  3 Train loss:  1.2783999698502677\n","K-Fold: 0, Epoch: 3, Accuracy: 0.4528301886792453, Recall: 0.4528301886792453, F1: 0.4528301886792453, Precision: 0.4528301886792453\n","K-Fold:  0 Epoch:  4 Train loss:  1.135295889207295\n","K-Fold: 0, Epoch: 4, Accuracy: 0.4056603773584906, Recall: 0.4056603773584906, F1: 0.4056603773584906, Precision: 0.4056603773584906\n","K-Fold:  0 Epoch:  5 Train loss:  1.0594872832298279\n","K-Fold: 0, Epoch: 5, Accuracy: 0.36792452830188677, Recall: 0.36792452830188677, F1: 0.36792452830188677, Precision: 0.36792452830188677\n","K-Fold:  0 Epoch:  6 Train loss:  0.9863166383334568\n","K-Fold: 0, Epoch: 6, Accuracy: 0.2830188679245283, Recall: 0.2830188679245283, F1: 0.2830188679245283, Precision: 0.2830188679245283\n","K-Fold:  0 Epoch:  7 Train loss:  0.9197684569018227\n","K-Fold: 0, Epoch: 7, Accuracy: 0.37735849056603776, Recall: 0.37735849056603776, F1: 0.3773584905660377, Precision: 0.37735849056603776\n","K-Fold:  0 Epoch:  8 Train loss:  0.7884742745331356\n","K-Fold: 0, Epoch: 8, Accuracy: 0.330188679245283, Recall: 0.330188679245283, F1: 0.330188679245283, Precision: 0.330188679245283\n","K-Fold:  0 Epoch:  9 Train loss:  0.7730406799486705\n","K-Fold: 0, Epoch: 9, Accuracy: 0.41509433962264153, Recall: 0.41509433962264153, F1: 0.41509433962264153, Precision: 0.41509433962264153\n","K-Fold:  0 Epoch:  10 Train loss:  0.7083359147821154\n","K-Fold: 0, Epoch: 10, Accuracy: 0.3490566037735849, Recall: 0.3490566037735849, F1: 0.3490566037735849, Precision: 0.3490566037735849\n","K-Fold:  0 Epoch:  11 Train loss:  0.6028612937246051\n","K-Fold: 0, Epoch: 11, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  0 Epoch:  12 Train loss:  0.4711534615073885\n","K-Fold: 0, Epoch: 12, Accuracy: 0.39622641509433965, Recall: 0.39622641509433965, F1: 0.39622641509433965, Precision: 0.39622641509433965\n","K-Fold:  0 Epoch:  13 Train loss:  0.514808072575501\n","K-Fold: 0, Epoch: 13, Accuracy: 0.6698113207547169, Recall: 0.6698113207547169, F1: 0.6698113207547169, Precision: 0.6698113207547169\n","K-Fold:  0 Epoch:  14 Train loss:  0.34292391687631607\n","K-Fold: 0, Epoch: 14, Accuracy: 0.5283018867924528, Recall: 0.5283018867924528, F1: 0.5283018867924528, Precision: 0.5283018867924528\n","K-Fold:  0 Epoch:  15 Train loss:  0.21873387534703528\n","K-Fold: 0, Epoch: 15, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  0 Epoch:  16 Train loss:  0.1504660900682211\n","K-Fold: 0, Epoch: 16, Accuracy: 0.5471698113207547, Recall: 0.5471698113207547, F1: 0.5471698113207547, Precision: 0.5471698113207547\n","K-Fold:  0 Epoch:  17 Train loss:  0.10329344575958592\n","K-Fold: 0, Epoch: 17, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  0 Epoch:  18 Train loss:  0.1335066947420793\n","K-Fold: 0, Epoch: 18, Accuracy: 0.6132075471698113, Recall: 0.6132075471698113, F1: 0.6132075471698113, Precision: 0.6132075471698113\n","K-Fold:  0 Epoch:  19 Train loss:  0.2556155951959746\n","K-Fold: 0, Epoch: 19, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  0 Epoch:  20 Train loss:  0.22887424752116203\n","K-Fold: 0, Epoch: 20, Accuracy: 0.39622641509433965, Recall: 0.39622641509433965, F1: 0.39622641509433965, Precision: 0.39622641509433965\n","K-Fold:  0 Epoch:  21 Train loss:  0.1969510723969766\n","K-Fold: 0, Epoch: 21, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  0 Epoch:  22 Train loss:  0.3063770215958357\n","K-Fold: 0, Epoch: 22, Accuracy: 0.6698113207547169, Recall: 0.6698113207547169, F1: 0.6698113207547169, Precision: 0.6698113207547169\n","K-Fold:  0 Epoch:  23 Train loss:  0.21033743396401405\n","K-Fold: 0, Epoch: 23, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  0 Epoch:  24 Train loss:  0.10052443854510784\n","K-Fold: 0, Epoch: 24, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  0 Epoch:  25 Train loss:  0.044526759535074234\n","K-Fold: 0, Epoch: 25, Accuracy: 0.7358490566037735, Recall: 0.7358490566037735, F1: 0.7358490566037735, Precision: 0.7358490566037735\n","K-Fold:  0 Epoch:  26 Train loss:  0.03531332044596119\n","K-Fold: 0, Epoch: 26, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  0 Epoch:  27 Train loss:  0.04354528126506401\n","K-Fold: 0, Epoch: 27, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  0 Epoch:  28 Train loss:  0.12593822866412147\n","K-Fold: 0, Epoch: 28, Accuracy: 0.4339622641509434, Recall: 0.4339622641509434, F1: 0.43396226415094347, Precision: 0.4339622641509434\n","K-Fold:  0 Epoch:  29 Train loss:  0.15090074938988046\n","K-Fold: 0, Epoch: 29, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  0 Epoch:  30 Train loss:  0.10978923857744251\n","K-Fold: 0, Epoch: 30, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  0 Epoch:  31 Train loss:  0.1460298356666629\n","K-Fold: 0, Epoch: 31, Accuracy: 0.5754716981132075, Recall: 0.5754716981132075, F1: 0.5754716981132075, Precision: 0.5754716981132075\n","K-Fold:  0 Epoch:  32 Train loss:  0.14259123542745197\n","K-Fold: 0, Epoch: 32, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  0 Epoch:  33 Train loss:  0.17158170616520302\n","K-Fold: 0, Epoch: 33, Accuracy: 0.5, Recall: 0.5, F1: 0.5, Precision: 0.5\n","K-Fold:  0 Epoch:  34 Train loss:  0.1503200700639614\n","K-Fold: 0, Epoch: 34, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  0 Epoch:  35 Train loss:  0.25314800567658885\n","K-Fold: 0, Epoch: 35, Accuracy: 0.5471698113207547, Recall: 0.5471698113207547, F1: 0.5471698113207547, Precision: 0.5471698113207547\n","K-Fold:  0 Epoch:  36 Train loss:  0.10191093465047223\n","K-Fold: 0, Epoch: 36, Accuracy: 0.6037735849056604, Recall: 0.6037735849056604, F1: 0.6037735849056604, Precision: 0.6037735849056604\n","K-Fold:  0 Epoch:  37 Train loss:  0.0539197141105043\n","K-Fold: 0, Epoch: 37, Accuracy: 0.6792452830188679, Recall: 0.6792452830188679, F1: 0.6792452830188679, Precision: 0.6792452830188679\n","K-Fold:  0 Epoch:  38 Train loss:  0.02109455912640052\n","K-Fold: 0, Epoch: 38, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  39 Train loss:  0.014586056798829563\n","K-Fold: 0, Epoch: 39, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  40 Train loss:  0.035695047417123406\n","K-Fold: 0, Epoch: 40, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  0 Epoch:  41 Train loss:  0.020383456256240606\n","K-Fold: 0, Epoch: 41, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  42 Train loss:  0.016579135138142322\n","K-Fold: 0, Epoch: 42, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  43 Train loss:  0.047967626505331803\n","K-Fold: 0, Epoch: 43, Accuracy: 0.3584905660377358, Recall: 0.3584905660377358, F1: 0.3584905660377358, Precision: 0.3584905660377358\n","K-Fold:  0 Epoch:  44 Train loss:  0.02619147699026923\n","K-Fold: 0, Epoch: 44, Accuracy: 0.5, Recall: 0.5, F1: 0.5, Precision: 0.5\n","K-Fold:  0 Epoch:  45 Train loss:  0.014375497769963528\n","K-Fold: 0, Epoch: 45, Accuracy: 0.7075471698113207, Recall: 0.7075471698113207, F1: 0.7075471698113207, Precision: 0.7075471698113207\n","K-Fold:  0 Epoch:  46 Train loss:  0.03646118697776858\n","K-Fold: 0, Epoch: 46, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  47 Train loss:  0.1021513265358018\n","K-Fold: 0, Epoch: 47, Accuracy: 0.5471698113207547, Recall: 0.5471698113207547, F1: 0.5471698113207547, Precision: 0.5471698113207547\n","K-Fold:  0 Epoch:  48 Train loss:  0.29365943066243616\n","K-Fold: 0, Epoch: 48, Accuracy: 0.5094339622641509, Recall: 0.5094339622641509, F1: 0.5094339622641509, Precision: 0.5094339622641509\n","K-Fold:  0 Epoch:  49 Train loss:  0.1955206830586706\n","K-Fold: 0, Epoch: 49, Accuracy: 0.5283018867924528, Recall: 0.5283018867924528, F1: 0.5283018867924528, Precision: 0.5283018867924528\n","K-Fold:  0 Epoch:  50 Train loss:  0.11442204391849893\n","K-Fold: 0, Epoch: 50, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  0 Epoch:  51 Train loss:  0.11218479721407805\n","K-Fold: 0, Epoch: 51, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  0 Epoch:  52 Train loss:  0.07301081850060395\n","K-Fold: 0, Epoch: 52, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  0 Epoch:  53 Train loss:  0.02267098213945116\n","K-Fold: 0, Epoch: 53, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  54 Train loss:  0.010782772583687412\n","K-Fold: 0, Epoch: 54, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  55 Train loss:  0.0037949004950600545\n","K-Fold: 0, Epoch: 55, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  0 Epoch:  56 Train loss:  0.0022527111494647606\n","K-Fold: 0, Epoch: 56, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  57 Train loss:  0.0008779171475907788\n","K-Fold: 0, Epoch: 57, Accuracy: 0.7075471698113207, Recall: 0.7075471698113207, F1: 0.7075471698113207, Precision: 0.7075471698113207\n","K-Fold:  0 Epoch:  58 Train loss:  0.0018603460796709572\n","K-Fold: 0, Epoch: 58, Accuracy: 0.7358490566037735, Recall: 0.7358490566037735, F1: 0.7358490566037735, Precision: 0.7358490566037735\n","K-Fold:  0 Epoch:  59 Train loss:  0.0010374124893652542\n","K-Fold: 0, Epoch: 59, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  60 Train loss:  0.0008175738403224386\n","K-Fold: 0, Epoch: 60, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  61 Train loss:  0.0010429971443954855\n","K-Fold: 0, Epoch: 61, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  62 Train loss:  0.0005194542110465201\n","K-Fold: 0, Epoch: 62, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  0 Epoch:  63 Train loss:  0.0005330868017543773\n","K-Fold: 0, Epoch: 63, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  0 Epoch:  64 Train loss:  0.0005120052311602714\n","K-Fold: 0, Epoch: 64, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  65 Train loss:  0.0006371525024795639\n","K-Fold: 0, Epoch: 65, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  0 Epoch:  66 Train loss:  0.0005041088105437146\n","K-Fold: 0, Epoch: 66, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  0 Epoch:  67 Train loss:  0.0005215142123883457\n","K-Fold: 0, Epoch: 67, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  68 Train loss:  0.0005569205069540269\n","K-Fold: 0, Epoch: 68, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  0 Epoch:  69 Train loss:  0.00019144260555289553\n","K-Fold: 0, Epoch: 69, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  70 Train loss:  0.0003197360820195172\n","K-Fold: 0, Epoch: 70, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  71 Train loss:  0.0003150384862757554\n","K-Fold: 0, Epoch: 71, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  0 Epoch:  72 Train loss:  0.0006508036983307518\n","K-Fold: 0, Epoch: 72, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  73 Train loss:  0.00023560948414212492\n","K-Fold: 0, Epoch: 73, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  0 Epoch:  74 Train loss:  0.0003230203905591874\n","K-Fold: 0, Epoch: 74, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  75 Train loss:  0.03856621986259207\n","K-Fold: 0, Epoch: 75, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  0 Epoch:  76 Train loss:  0.38041947564696393\n","K-Fold: 0, Epoch: 76, Accuracy: 0.46226415094339623, Recall: 0.46226415094339623, F1: 0.46226415094339623, Precision: 0.46226415094339623\n","K-Fold:  0 Epoch:  77 Train loss:  0.15692851745656558\n","K-Fold: 0, Epoch: 77, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  0 Epoch:  78 Train loss:  0.101022859636162\n","K-Fold: 0, Epoch: 78, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  0 Epoch:  79 Train loss:  0.045175893182334094\n","K-Fold: 0, Epoch: 79, Accuracy: 0.6886792452830188, Recall: 0.6886792452830188, F1: 0.6886792452830188, Precision: 0.6886792452830188\n","K-Fold:  0 Epoch:  80 Train loss:  0.014251587430148252\n","K-Fold: 0, Epoch: 80, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  0 Epoch:  81 Train loss:  0.07700954172261325\n","K-Fold: 0, Epoch: 81, Accuracy: 0.7358490566037735, Recall: 0.7358490566037735, F1: 0.7358490566037735, Precision: 0.7358490566037735\n","K-Fold:  0 Epoch:  82 Train loss:  0.04332893724702964\n","K-Fold: 0, Epoch: 82, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  0 Epoch:  83 Train loss:  0.12354835462922763\n","K-Fold: 0, Epoch: 83, Accuracy: 0.4528301886792453, Recall: 0.4528301886792453, F1: 0.4528301886792453, Precision: 0.4528301886792453\n","K-Fold:  0 Epoch:  84 Train loss:  0.3879397953195231\n","K-Fold: 0, Epoch: 84, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  0 Epoch:  85 Train loss:  0.215057343909783\n","K-Fold: 0, Epoch: 85, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  0 Epoch:  86 Train loss:  0.0655903812231762\n","K-Fold: 0, Epoch: 86, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  0 Epoch:  87 Train loss:  0.018922517408749888\n","K-Fold: 0, Epoch: 87, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  0 Epoch:  88 Train loss:  0.005463748180773109\n","K-Fold: 0, Epoch: 88, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  89 Train loss:  0.0028828798863937016\n","K-Fold: 0, Epoch: 89, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  90 Train loss:  0.0007001554778461079\n","K-Fold: 0, Epoch: 90, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  91 Train loss:  0.0009929574152920395\n","K-Fold: 0, Epoch: 91, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  92 Train loss:  0.005121275965523507\n","K-Fold: 0, Epoch: 92, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  0 Epoch:  93 Train loss:  0.002459624878870922\n","K-Fold: 0, Epoch: 93, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  0 Epoch:  94 Train loss:  0.0018945095975401013\n","K-Fold: 0, Epoch: 94, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  0 Epoch:  95 Train loss:  0.021547815738553515\n","K-Fold: 0, Epoch: 95, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  0 Epoch:  96 Train loss:  0.10421738623907524\n","K-Fold: 0, Epoch: 96, Accuracy: 0.5188679245283019, Recall: 0.5188679245283019, F1: 0.5188679245283019, Precision: 0.5188679245283019\n","K-Fold:  0 Epoch:  97 Train loss:  0.16574148010529047\n","K-Fold: 0, Epoch: 97, Accuracy: 0.5943396226415094, Recall: 0.5943396226415094, F1: 0.5943396226415094, Precision: 0.5943396226415094\n","K-Fold:  0 Epoch:  98 Train loss:  0.1307453641534916\n","K-Fold: 0, Epoch: 98, Accuracy: 0.6132075471698113, Recall: 0.6132075471698113, F1: 0.6132075471698113, Precision: 0.6132075471698113\n","K-Fold:  0 Epoch:  99 Train loss:  0.027038429265043566\n","K-Fold: 0, Epoch: 99, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  1 Epoch:  0 Train loss:  1.5008314081600733\n","K-Fold: 1, Epoch: 0, Accuracy: 0.41509433962264153, Recall: 0.41509433962264153, F1: 0.41509433962264153, Precision: 0.41509433962264153\n","K-Fold:  1 Epoch:  1 Train loss:  1.3716569202286857\n","K-Fold: 1, Epoch: 1, Accuracy: 0.27358490566037735, Recall: 0.27358490566037735, F1: 0.27358490566037735, Precision: 0.27358490566037735\n","K-Fold:  1 Epoch:  2 Train loss:  1.179697837148394\n","K-Fold: 1, Epoch: 2, Accuracy: 0.5377358490566038, Recall: 0.5377358490566038, F1: 0.5377358490566038, Precision: 0.5377358490566038\n","K-Fold:  1 Epoch:  3 Train loss:  1.0005379234041487\n","K-Fold: 1, Epoch: 3, Accuracy: 0.5377358490566038, Recall: 0.5377358490566038, F1: 0.5377358490566038, Precision: 0.5377358490566038\n","K-Fold:  1 Epoch:  4 Train loss:  0.925692754132407\n","K-Fold: 1, Epoch: 4, Accuracy: 0.33962264150943394, Recall: 0.33962264150943394, F1: 0.33962264150943394, Precision: 0.33962264150943394\n","K-Fold:  1 Epoch:  5 Train loss:  1.0088946521282196\n","K-Fold: 1, Epoch: 5, Accuracy: 0.5, Recall: 0.5, F1: 0.5, Precision: 0.5\n","K-Fold:  1 Epoch:  6 Train loss:  0.8735031698431287\n","K-Fold: 1, Epoch: 6, Accuracy: 0.330188679245283, Recall: 0.330188679245283, F1: 0.330188679245283, Precision: 0.330188679245283\n","K-Fold:  1 Epoch:  7 Train loss:  0.8835516784872327\n","K-Fold: 1, Epoch: 7, Accuracy: 0.5283018867924528, Recall: 0.5283018867924528, F1: 0.5283018867924528, Precision: 0.5283018867924528\n","K-Fold:  1 Epoch:  8 Train loss:  0.7025094415460315\n","K-Fold: 1, Epoch: 8, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  1 Epoch:  9 Train loss:  0.7090551427432469\n","K-Fold: 1, Epoch: 9, Accuracy: 0.5094339622641509, Recall: 0.5094339622641509, F1: 0.5094339622641509, Precision: 0.5094339622641509\n","K-Fold:  1 Epoch:  10 Train loss:  0.6367600858211517\n","K-Fold: 1, Epoch: 10, Accuracy: 0.49056603773584906, Recall: 0.49056603773584906, F1: 0.49056603773584906, Precision: 0.49056603773584906\n","K-Fold:  1 Epoch:  11 Train loss:  0.795709661075047\n","K-Fold: 1, Epoch: 11, Accuracy: 0.32075471698113206, Recall: 0.32075471698113206, F1: 0.32075471698113206, Precision: 0.32075471698113206\n","K-Fold:  1 Epoch:  12 Train loss:  0.5590029252426965\n","K-Fold: 1, Epoch: 12, Accuracy: 0.6132075471698113, Recall: 0.6132075471698113, F1: 0.6132075471698113, Precision: 0.6132075471698113\n","K-Fold:  1 Epoch:  13 Train loss:  0.4046979227236339\n","K-Fold: 1, Epoch: 13, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  1 Epoch:  14 Train loss:  0.3839772811957768\n","K-Fold: 1, Epoch: 14, Accuracy: 0.6886792452830188, Recall: 0.6886792452830188, F1: 0.6886792452830188, Precision: 0.6886792452830188\n","K-Fold:  1 Epoch:  15 Train loss:  0.3794482850602695\n","K-Fold: 1, Epoch: 15, Accuracy: 0.6320754716981132, Recall: 0.6320754716981132, F1: 0.6320754716981132, Precision: 0.6320754716981132\n","K-Fold:  1 Epoch:  16 Train loss:  0.3052095366375787\n","K-Fold: 1, Epoch: 16, Accuracy: 0.24528301886792453, Recall: 0.24528301886792453, F1: 0.24528301886792453, Precision: 0.24528301886792453\n","K-Fold:  1 Epoch:  17 Train loss:  0.273652476391622\n","K-Fold: 1, Epoch: 17, Accuracy: 0.5660377358490566, Recall: 0.5660377358490566, F1: 0.5660377358490566, Precision: 0.5660377358490566\n","K-Fold:  1 Epoch:  18 Train loss:  0.4691938725965364\n","K-Fold: 1, Epoch: 18, Accuracy: 0.39622641509433965, Recall: 0.39622641509433965, F1: 0.39622641509433965, Precision: 0.39622641509433965\n","K-Fold:  1 Epoch:  19 Train loss:  0.29247759974428583\n","K-Fold: 1, Epoch: 19, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  1 Epoch:  20 Train loss:  0.1573538766907794\n","K-Fold: 1, Epoch: 20, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  1 Epoch:  21 Train loss:  0.1921813583799771\n","K-Fold: 1, Epoch: 21, Accuracy: 0.27358490566037735, Recall: 0.27358490566037735, F1: 0.27358490566037735, Precision: 0.27358490566037735\n","K-Fold:  1 Epoch:  22 Train loss:  0.46751569850104197\n","K-Fold: 1, Epoch: 22, Accuracy: 0.37735849056603776, Recall: 0.37735849056603776, F1: 0.3773584905660377, Precision: 0.37735849056603776\n","K-Fold:  1 Epoch:  23 Train loss:  0.4585174684013639\n","K-Fold: 1, Epoch: 23, Accuracy: 0.4811320754716981, Recall: 0.4811320754716981, F1: 0.4811320754716981, Precision: 0.4811320754716981\n","K-Fold:  1 Epoch:  24 Train loss:  0.38139863312244415\n","K-Fold: 1, Epoch: 24, Accuracy: 0.6226415094339622, Recall: 0.6226415094339622, F1: 0.6226415094339622, Precision: 0.6226415094339622\n","K-Fold:  1 Epoch:  25 Train loss:  0.29661289868610247\n","K-Fold: 1, Epoch: 25, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  1 Epoch:  26 Train loss:  0.11975541364933763\n","K-Fold: 1, Epoch: 26, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  1 Epoch:  27 Train loss:  0.08096390949296099\n","K-Fold: 1, Epoch: 27, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  1 Epoch:  28 Train loss:  0.06600233500025102\n","K-Fold: 1, Epoch: 28, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  1 Epoch:  29 Train loss:  0.056740052731973786\n","K-Fold: 1, Epoch: 29, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  1 Epoch:  30 Train loss:  0.02340585016645491\n","K-Fold: 1, Epoch: 30, Accuracy: 0.6792452830188679, Recall: 0.6792452830188679, F1: 0.6792452830188679, Precision: 0.6792452830188679\n","K-Fold:  1 Epoch:  31 Train loss:  0.032336233516356776\n","K-Fold: 1, Epoch: 31, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  1 Epoch:  32 Train loss:  0.03224013429800315\n","K-Fold: 1, Epoch: 32, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  1 Epoch:  33 Train loss:  0.04568363147388611\n","K-Fold: 1, Epoch: 33, Accuracy: 0.4811320754716981, Recall: 0.4811320754716981, F1: 0.4811320754716981, Precision: 0.4811320754716981\n","K-Fold:  1 Epoch:  34 Train loss:  0.04387249837496451\n","K-Fold: 1, Epoch: 34, Accuracy: 0.5754716981132075, Recall: 0.5754716981132075, F1: 0.5754716981132075, Precision: 0.5754716981132075\n","K-Fold:  1 Epoch:  35 Train loss:  0.06809952748673302\n","K-Fold: 1, Epoch: 35, Accuracy: 0.6226415094339622, Recall: 0.6226415094339622, F1: 0.6226415094339622, Precision: 0.6226415094339622\n","K-Fold:  1 Epoch:  36 Train loss:  0.06568550231999584\n","K-Fold: 1, Epoch: 36, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  1 Epoch:  37 Train loss:  0.08779443181785089\n","K-Fold: 1, Epoch: 37, Accuracy: 0.4811320754716981, Recall: 0.4811320754716981, F1: 0.4811320754716981, Precision: 0.4811320754716981\n","K-Fold:  1 Epoch:  38 Train loss:  0.16889969735140248\n","K-Fold: 1, Epoch: 38, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  1 Epoch:  39 Train loss:  0.14931116718798876\n","K-Fold: 1, Epoch: 39, Accuracy: 0.4339622641509434, Recall: 0.4339622641509434, F1: 0.43396226415094347, Precision: 0.4339622641509434\n","K-Fold:  1 Epoch:  40 Train loss:  0.09751195320859551\n","K-Fold: 1, Epoch: 40, Accuracy: 0.5188679245283019, Recall: 0.5188679245283019, F1: 0.5188679245283019, Precision: 0.5188679245283019\n","K-Fold:  1 Epoch:  41 Train loss:  0.058186588542801995\n","K-Fold: 1, Epoch: 41, Accuracy: 0.6886792452830188, Recall: 0.6886792452830188, F1: 0.6886792452830188, Precision: 0.6886792452830188\n","K-Fold:  1 Epoch:  42 Train loss:  0.07196421206130513\n","K-Fold: 1, Epoch: 42, Accuracy: 0.5377358490566038, Recall: 0.5377358490566038, F1: 0.5377358490566038, Precision: 0.5377358490566038\n","K-Fold:  1 Epoch:  43 Train loss:  0.029340051613481983\n","K-Fold: 1, Epoch: 43, Accuracy: 0.6415094339622641, Recall: 0.6415094339622641, F1: 0.6415094339622641, Precision: 0.6415094339622641\n","K-Fold:  1 Epoch:  44 Train loss:  0.10403300072565409\n","K-Fold: 1, Epoch: 44, Accuracy: 0.6037735849056604, Recall: 0.6037735849056604, F1: 0.6037735849056604, Precision: 0.6037735849056604\n","K-Fold:  1 Epoch:  45 Train loss:  0.24394761305302382\n","K-Fold: 1, Epoch: 45, Accuracy: 0.46226415094339623, Recall: 0.46226415094339623, F1: 0.46226415094339623, Precision: 0.46226415094339623\n","K-Fold:  1 Epoch:  46 Train loss:  0.1844082486017474\n","K-Fold: 1, Epoch: 46, Accuracy: 0.46226415094339623, Recall: 0.46226415094339623, F1: 0.46226415094339623, Precision: 0.46226415094339623\n","K-Fold:  1 Epoch:  47 Train loss:  0.10840057814493775\n","K-Fold: 1, Epoch: 47, Accuracy: 0.6886792452830188, Recall: 0.6886792452830188, F1: 0.6886792452830188, Precision: 0.6886792452830188\n","K-Fold:  1 Epoch:  48 Train loss:  0.06246062178563859\n","K-Fold: 1, Epoch: 48, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  1 Epoch:  49 Train loss:  0.07005515606475196\n","K-Fold: 1, Epoch: 49, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  1 Epoch:  50 Train loss:  0.2295107003301382\n","K-Fold: 1, Epoch: 50, Accuracy: 0.44339622641509435, Recall: 0.44339622641509435, F1: 0.44339622641509435, Precision: 0.44339622641509435\n","K-Fold:  1 Epoch:  51 Train loss:  0.15893554421407835\n","K-Fold: 1, Epoch: 51, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  1 Epoch:  52 Train loss:  0.0628789100529892\n","K-Fold: 1, Epoch: 52, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  1 Epoch:  53 Train loss:  0.03885120861897511\n","K-Fold: 1, Epoch: 53, Accuracy: 0.6037735849056604, Recall: 0.6037735849056604, F1: 0.6037735849056604, Precision: 0.6037735849056604\n","K-Fold:  1 Epoch:  54 Train loss:  0.01889173239136913\n","K-Fold: 1, Epoch: 54, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  1 Epoch:  55 Train loss:  0.018016827858186195\n","K-Fold: 1, Epoch: 55, Accuracy: 0.6792452830188679, Recall: 0.6792452830188679, F1: 0.6792452830188679, Precision: 0.6792452830188679\n","K-Fold:  1 Epoch:  56 Train loss:  0.01576092272132103\n","K-Fold: 1, Epoch: 56, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  1 Epoch:  57 Train loss:  0.008768917127911533\n","K-Fold: 1, Epoch: 57, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  1 Epoch:  58 Train loss:  0.00295774988314536\n","K-Fold: 1, Epoch: 58, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  1 Epoch:  59 Train loss:  0.005943582501328949\n","K-Fold: 1, Epoch: 59, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  60 Train loss:  0.004343330107596037\n","K-Fold: 1, Epoch: 60, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  61 Train loss:  0.006812708552128502\n","K-Fold: 1, Epoch: 61, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  1 Epoch:  62 Train loss:  0.01097042826170634\n","K-Fold: 1, Epoch: 62, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  1 Epoch:  63 Train loss:  0.04913643775112827\n","K-Fold: 1, Epoch: 63, Accuracy: 0.7075471698113207, Recall: 0.7075471698113207, F1: 0.7075471698113207, Precision: 0.7075471698113207\n","K-Fold:  1 Epoch:  64 Train loss:  0.1499655907557878\n","K-Fold: 1, Epoch: 64, Accuracy: 0.6226415094339622, Recall: 0.6226415094339622, F1: 0.6226415094339622, Precision: 0.6226415094339622\n","K-Fold:  1 Epoch:  65 Train loss:  0.046017666738147715\n","K-Fold: 1, Epoch: 65, Accuracy: 0.5754716981132075, Recall: 0.5754716981132075, F1: 0.5754716981132075, Precision: 0.5754716981132075\n","K-Fold:  1 Epoch:  66 Train loss:  0.0356279841092016\n","K-Fold: 1, Epoch: 66, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  1 Epoch:  67 Train loss:  0.03712914267920756\n","K-Fold: 1, Epoch: 67, Accuracy: 0.7358490566037735, Recall: 0.7358490566037735, F1: 0.7358490566037735, Precision: 0.7358490566037735\n","K-Fold:  1 Epoch:  68 Train loss:  0.03131067467620596\n","K-Fold: 1, Epoch: 68, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  1 Epoch:  69 Train loss:  0.08682485414569133\n","K-Fold: 1, Epoch: 69, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  1 Epoch:  70 Train loss:  0.042177003143089156\n","K-Fold: 1, Epoch: 70, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  1 Epoch:  71 Train loss:  0.029518332232588103\n","K-Fold: 1, Epoch: 71, Accuracy: 0.6792452830188679, Recall: 0.6792452830188679, F1: 0.6792452830188679, Precision: 0.6792452830188679\n","K-Fold:  1 Epoch:  72 Train loss:  0.012255939230921544\n","K-Fold: 1, Epoch: 72, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  1 Epoch:  73 Train loss:  0.005858428549670082\n","K-Fold: 1, Epoch: 73, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  1 Epoch:  74 Train loss:  0.002568840839168323\n","K-Fold: 1, Epoch: 74, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  1 Epoch:  75 Train loss:  0.0013619645760627463\n","K-Fold: 1, Epoch: 75, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  1 Epoch:  76 Train loss:  0.0007422099151881412\n","K-Fold: 1, Epoch: 76, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  1 Epoch:  77 Train loss:  0.0005016702198190615\n","K-Fold: 1, Epoch: 77, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  1 Epoch:  78 Train loss:  0.0005902172103690516\n","K-Fold: 1, Epoch: 78, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  79 Train loss:  0.0023189364604019958\n","K-Fold: 1, Epoch: 79, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  1 Epoch:  80 Train loss:  0.0013462725712867854\n","K-Fold: 1, Epoch: 80, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  1 Epoch:  81 Train loss:  0.0015982533929803009\n","K-Fold: 1, Epoch: 81, Accuracy: 0.8207547169811321, Recall: 0.8207547169811321, F1: 0.8207547169811321, Precision: 0.8207547169811321\n","K-Fold:  1 Epoch:  82 Train loss:  0.0013236510323102785\n","K-Fold: 1, Epoch: 82, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  1 Epoch:  83 Train loss:  0.000728894354064583\n","K-Fold: 1, Epoch: 83, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  1 Epoch:  84 Train loss:  0.06703141165520979\n","K-Fold: 1, Epoch: 84, Accuracy: 0.5849056603773585, Recall: 0.5849056603773585, F1: 0.5849056603773585, Precision: 0.5849056603773585\n","K-Fold:  1 Epoch:  85 Train loss:  0.3370137768132346\n","K-Fold: 1, Epoch: 85, Accuracy: 0.5, Recall: 0.5, F1: 0.5, Precision: 0.5\n","K-Fold:  1 Epoch:  86 Train loss:  0.3133281748741865\n","K-Fold: 1, Epoch: 86, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  1 Epoch:  87 Train loss:  0.11670915795756238\n","K-Fold: 1, Epoch: 87, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  1 Epoch:  88 Train loss:  0.1574379046381052\n","K-Fold: 1, Epoch: 88, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  1 Epoch:  89 Train loss:  0.16278346428381546\n","K-Fold: 1, Epoch: 89, Accuracy: 0.6415094339622641, Recall: 0.6415094339622641, F1: 0.6415094339622641, Precision: 0.6415094339622641\n","K-Fold:  1 Epoch:  90 Train loss:  0.07739655633590051\n","K-Fold: 1, Epoch: 90, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  1 Epoch:  91 Train loss:  0.05317675501906446\n","K-Fold: 1, Epoch: 91, Accuracy: 0.660377358490566, Recall: 0.660377358490566, F1: 0.660377358490566, Precision: 0.660377358490566\n","K-Fold:  1 Epoch:  92 Train loss:  0.02312421003755714\n","K-Fold: 1, Epoch: 92, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  1 Epoch:  93 Train loss:  0.009102672538054841\n","K-Fold: 1, Epoch: 93, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  1 Epoch:  94 Train loss:  0.0041231041915515175\n","K-Fold: 1, Epoch: 94, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  1 Epoch:  95 Train loss:  0.0017084727670797811\n","K-Fold: 1, Epoch: 95, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  1 Epoch:  96 Train loss:  0.001967701582803524\n","K-Fold: 1, Epoch: 96, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  97 Train loss:  0.0014482123619278095\n","K-Fold: 1, Epoch: 97, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  98 Train loss:  0.0021626773772628178\n","K-Fold: 1, Epoch: 98, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  1 Epoch:  99 Train loss:  0.0017648034201036872\n","K-Fold: 1, Epoch: 99, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  2 Epoch:  0 Train loss:  1.5454008408955164\n","K-Fold: 2, Epoch: 0, Accuracy: 0.29245283018867924, Recall: 0.29245283018867924, F1: 0.29245283018867924, Precision: 0.29245283018867924\n","K-Fold:  2 Epoch:  1 Train loss:  1.3720013414110457\n","K-Fold: 2, Epoch: 1, Accuracy: 0.32075471698113206, Recall: 0.32075471698113206, F1: 0.32075471698113206, Precision: 0.32075471698113206\n","K-Fold:  2 Epoch:  2 Train loss:  1.357010109083993\n","K-Fold: 2, Epoch: 2, Accuracy: 0.41509433962264153, Recall: 0.41509433962264153, F1: 0.41509433962264153, Precision: 0.41509433962264153\n","K-Fold:  2 Epoch:  3 Train loss:  1.3088837776865279\n","K-Fold: 2, Epoch: 3, Accuracy: 0.4528301886792453, Recall: 0.4528301886792453, F1: 0.4528301886792453, Precision: 0.4528301886792453\n","K-Fold:  2 Epoch:  4 Train loss:  1.2525345172200884\n","K-Fold: 2, Epoch: 4, Accuracy: 0.3867924528301887, Recall: 0.3867924528301887, F1: 0.38679245283018876, Precision: 0.3867924528301887\n","K-Fold:  2 Epoch:  5 Train loss:  1.2297636270523071\n","K-Fold: 2, Epoch: 5, Accuracy: 0.46226415094339623, Recall: 0.46226415094339623, F1: 0.46226415094339623, Precision: 0.46226415094339623\n","K-Fold:  2 Epoch:  6 Train loss:  1.1185041453157152\n","K-Fold: 2, Epoch: 6, Accuracy: 0.5094339622641509, Recall: 0.5094339622641509, F1: 0.5094339622641509, Precision: 0.5094339622641509\n","K-Fold:  2 Epoch:  7 Train loss:  1.1729266515799932\n","K-Fold: 2, Epoch: 7, Accuracy: 0.3490566037735849, Recall: 0.3490566037735849, F1: 0.3490566037735849, Precision: 0.3490566037735849\n","K-Fold:  2 Epoch:  8 Train loss:  1.0914683469704218\n","K-Fold: 2, Epoch: 8, Accuracy: 0.5094339622641509, Recall: 0.5094339622641509, F1: 0.5094339622641509, Precision: 0.5094339622641509\n","K-Fold:  2 Epoch:  9 Train loss:  0.9842801945550101\n","K-Fold: 2, Epoch: 9, Accuracy: 0.5660377358490566, Recall: 0.5660377358490566, F1: 0.5660377358490566, Precision: 0.5660377358490566\n","K-Fold:  2 Epoch:  10 Train loss:  0.9245384803840092\n","K-Fold: 2, Epoch: 10, Accuracy: 0.22641509433962265, Recall: 0.22641509433962265, F1: 0.22641509433962265, Precision: 0.22641509433962265\n","K-Fold:  2 Epoch:  11 Train loss:  0.877449801989964\n","K-Fold: 2, Epoch: 11, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  2 Epoch:  12 Train loss:  0.8070482185908726\n","K-Fold: 2, Epoch: 12, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  2 Epoch:  13 Train loss:  0.7080735457795007\n","K-Fold: 2, Epoch: 13, Accuracy: 0.5660377358490566, Recall: 0.5660377358490566, F1: 0.5660377358490566, Precision: 0.5660377358490566\n","K-Fold:  2 Epoch:  14 Train loss:  0.4832956556762968\n","K-Fold: 2, Epoch: 14, Accuracy: 0.6226415094339622, Recall: 0.6226415094339622, F1: 0.6226415094339622, Precision: 0.6226415094339622\n","K-Fold:  2 Epoch:  15 Train loss:  0.36156695229666574\n","K-Fold: 2, Epoch: 15, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  16 Train loss:  0.3044094124010631\n","K-Fold: 2, Epoch: 16, Accuracy: 0.4528301886792453, Recall: 0.4528301886792453, F1: 0.4528301886792453, Precision: 0.4528301886792453\n","K-Fold:  2 Epoch:  17 Train loss:  0.27632846949355944\n","K-Fold: 2, Epoch: 17, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  2 Epoch:  18 Train loss:  0.3158084994980267\n","K-Fold: 2, Epoch: 18, Accuracy: 0.29245283018867924, Recall: 0.29245283018867924, F1: 0.29245283018867924, Precision: 0.29245283018867924\n","K-Fold:  2 Epoch:  19 Train loss:  0.4249257647565433\n","K-Fold: 2, Epoch: 19, Accuracy: 0.39622641509433965, Recall: 0.39622641509433965, F1: 0.39622641509433965, Precision: 0.39622641509433965\n","K-Fold:  2 Epoch:  20 Train loss:  0.2680114121841533\n","K-Fold: 2, Epoch: 20, Accuracy: 0.5471698113207547, Recall: 0.5471698113207547, F1: 0.5471698113207547, Precision: 0.5471698113207547\n","K-Fold:  2 Epoch:  21 Train loss:  0.19755211392683641\n","K-Fold: 2, Epoch: 21, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  2 Epoch:  22 Train loss:  0.17000720543520792\n","K-Fold: 2, Epoch: 22, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  2 Epoch:  23 Train loss:  0.18359975277313165\n","K-Fold: 2, Epoch: 23, Accuracy: 0.5094339622641509, Recall: 0.5094339622641509, F1: 0.5094339622641509, Precision: 0.5094339622641509\n","K-Fold:  2 Epoch:  24 Train loss:  0.17161757006709064\n","K-Fold: 2, Epoch: 24, Accuracy: 0.5283018867924528, Recall: 0.5283018867924528, F1: 0.5283018867924528, Precision: 0.5283018867924528\n","K-Fold:  2 Epoch:  25 Train loss:  0.1890616604526128\n","K-Fold: 2, Epoch: 25, Accuracy: 0.6698113207547169, Recall: 0.6698113207547169, F1: 0.6698113207547169, Precision: 0.6698113207547169\n","K-Fold:  2 Epoch:  26 Train loss:  0.24875088395284756\n","K-Fold: 2, Epoch: 26, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  27 Train loss:  0.07429463476208704\n","K-Fold: 2, Epoch: 27, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  2 Epoch:  28 Train loss:  0.05595005649541106\n","K-Fold: 2, Epoch: 28, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  29 Train loss:  0.05598656540470464\n","K-Fold: 2, Epoch: 29, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  2 Epoch:  30 Train loss:  0.1854588649501758\n","K-Fold: 2, Epoch: 30, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  2 Epoch:  31 Train loss:  0.15974142769950309\n","K-Fold: 2, Epoch: 31, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  2 Epoch:  32 Train loss:  0.06022247126592057\n","K-Fold: 2, Epoch: 32, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  2 Epoch:  33 Train loss:  0.07082033077520984\n","K-Fold: 2, Epoch: 33, Accuracy: 0.7641509433962265, Recall: 0.7641509433962265, F1: 0.7641509433962265, Precision: 0.7641509433962265\n","K-Fold:  2 Epoch:  34 Train loss:  0.29160034091078807\n","K-Fold: 2, Epoch: 34, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  2 Epoch:  35 Train loss:  0.09715220278927258\n","K-Fold: 2, Epoch: 35, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  36 Train loss:  0.04668089035632355\n","K-Fold: 2, Epoch: 36, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  2 Epoch:  37 Train loss:  0.02742071872177933\n","K-Fold: 2, Epoch: 37, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  38 Train loss:  0.07265497182795246\n","K-Fold: 2, Epoch: 38, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  2 Epoch:  39 Train loss:  0.03425476879679731\n","K-Fold: 2, Epoch: 39, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  40 Train loss:  0.02600423213360565\n","K-Fold: 2, Epoch: 40, Accuracy: 0.7547169811320755, Recall: 0.7547169811320755, F1: 0.7547169811320754, Precision: 0.7547169811320755\n","K-Fold:  2 Epoch:  41 Train loss:  0.008665877909931754\n","K-Fold: 2, Epoch: 41, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  2 Epoch:  42 Train loss:  0.007620159203984908\n","K-Fold: 2, Epoch: 42, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  2 Epoch:  43 Train loss:  0.008492036444035225\n","K-Fold: 2, Epoch: 43, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  2 Epoch:  44 Train loss:  0.016320786804759076\n","K-Fold: 2, Epoch: 44, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  45 Train loss:  0.016148837719811127\n","K-Fold: 2, Epoch: 45, Accuracy: 0.6981132075471698, Recall: 0.6981132075471698, F1: 0.6981132075471698, Precision: 0.6981132075471698\n","K-Fold:  2 Epoch:  46 Train loss:  0.008879703773085825\n","K-Fold: 2, Epoch: 46, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  2 Epoch:  47 Train loss:  0.011943144966997872\n","K-Fold: 2, Epoch: 47, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  48 Train loss:  0.01652724139109653\n","K-Fold: 2, Epoch: 48, Accuracy: 0.8207547169811321, Recall: 0.8207547169811321, F1: 0.8207547169811321, Precision: 0.8207547169811321\n","K-Fold:  2 Epoch:  49 Train loss:  0.01909188086886258\n","K-Fold: 2, Epoch: 49, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  2 Epoch:  50 Train loss:  0.035026801454867904\n","K-Fold: 2, Epoch: 50, Accuracy: 0.6509433962264151, Recall: 0.6509433962264151, F1: 0.6509433962264151, Precision: 0.6509433962264151\n","K-Fold:  2 Epoch:  51 Train loss:  0.024300537211404714\n","K-Fold: 2, Epoch: 51, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  52 Train loss:  0.039228979589617144\n","K-Fold: 2, Epoch: 52, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  2 Epoch:  53 Train loss:  0.009328723236519312\n","K-Fold: 2, Epoch: 53, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  2 Epoch:  54 Train loss:  0.037711584032097435\n","K-Fold: 2, Epoch: 54, Accuracy: 0.7075471698113207, Recall: 0.7075471698113207, F1: 0.7075471698113207, Precision: 0.7075471698113207\n","K-Fold:  2 Epoch:  55 Train loss:  0.2669526234136096\n","K-Fold: 2, Epoch: 55, Accuracy: 0.7075471698113207, Recall: 0.7075471698113207, F1: 0.7075471698113207, Precision: 0.7075471698113207\n","K-Fold:  2 Epoch:  56 Train loss:  0.14762134650456055\n","K-Fold: 2, Epoch: 56, Accuracy: 0.7264150943396226, Recall: 0.7264150943396226, F1: 0.7264150943396227, Precision: 0.7264150943396226\n","K-Fold:  2 Epoch:  57 Train loss:  0.13209210269685304\n","K-Fold: 2, Epoch: 57, Accuracy: 0.4811320754716981, Recall: 0.4811320754716981, F1: 0.4811320754716981, Precision: 0.4811320754716981\n","K-Fold:  2 Epoch:  58 Train loss:  0.28071766290148453\n","K-Fold: 2, Epoch: 58, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  59 Train loss:  0.15120425700609172\n","K-Fold: 2, Epoch: 59, Accuracy: 0.7452830188679245, Recall: 0.7452830188679245, F1: 0.7452830188679244, Precision: 0.7452830188679245\n","K-Fold:  2 Epoch:  60 Train loss:  0.186704254443092\n","K-Fold: 2, Epoch: 60, Accuracy: 0.4339622641509434, Recall: 0.4339622641509434, F1: 0.43396226415094347, Precision: 0.4339622641509434\n","K-Fold:  2 Epoch:  61 Train loss:  0.11315065981554133\n","K-Fold: 2, Epoch: 61, Accuracy: 0.7830188679245284, Recall: 0.7830188679245284, F1: 0.7830188679245284, Precision: 0.7830188679245284\n","K-Fold:  2 Epoch:  62 Train loss:  0.08490571328106203\n","K-Fold: 2, Epoch: 62, Accuracy: 0.7169811320754716, Recall: 0.7169811320754716, F1: 0.7169811320754716, Precision: 0.7169811320754716\n","K-Fold:  2 Epoch:  63 Train loss:  0.12129990367351898\n","K-Fold: 2, Epoch: 63, Accuracy: 0.5943396226415094, Recall: 0.5943396226415094, F1: 0.5943396226415094, Precision: 0.5943396226415094\n","K-Fold:  2 Epoch:  64 Train loss:  0.025140923048768724\n","K-Fold: 2, Epoch: 64, Accuracy: 0.7735849056603774, Recall: 0.7735849056603774, F1: 0.7735849056603775, Precision: 0.7735849056603774\n","K-Fold:  2 Epoch:  65 Train loss:  0.021527305794214562\n","K-Fold: 2, Epoch: 65, Accuracy: 0.8207547169811321, Recall: 0.8207547169811321, F1: 0.8207547169811321, Precision: 0.8207547169811321\n","K-Fold:  2 Epoch:  66 Train loss:  0.007000090687402657\n","K-Fold: 2, Epoch: 66, Accuracy: 0.8301886792452831, Recall: 0.8301886792452831, F1: 0.8301886792452831, Precision: 0.8301886792452831\n","K-Fold:  2 Epoch:  67 Train loss:  0.005022378987632692\n","K-Fold: 2, Epoch: 67, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  68 Train loss:  0.006785856362382349\n","K-Fold: 2, Epoch: 68, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  69 Train loss:  0.0021554705537190394\n","K-Fold: 2, Epoch: 69, Accuracy: 0.7924528301886793, Recall: 0.7924528301886793, F1: 0.7924528301886793, Precision: 0.7924528301886793\n","K-Fold:  2 Epoch:  70 Train loss:  0.0020634419342968613\n","K-Fold: 2, Epoch: 70, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  71 Train loss:  0.0014446322727183411\n","K-Fold: 2, Epoch: 71, Accuracy: 0.8490566037735849, Recall: 0.8490566037735849, F1: 0.8490566037735849, Precision: 0.8490566037735849\n","K-Fold:  2 Epoch:  72 Train loss:  0.000872158030688297\n","K-Fold: 2, Epoch: 72, Accuracy: 0.8867924528301887, Recall: 0.8867924528301887, F1: 0.8867924528301887, Precision: 0.8867924528301887\n","K-Fold:  2 Epoch:  73 Train loss:  0.0009422409233853354\n","K-Fold: 2, Epoch: 73, Accuracy: 0.8679245283018868, Recall: 0.8679245283018868, F1: 0.8679245283018869, Precision: 0.8679245283018868\n","K-Fold:  2 Epoch:  74 Train loss:  0.000457491370720423\n","K-Fold: 2, Epoch: 74, Accuracy: 0.8867924528301887, Recall: 0.8867924528301887, F1: 0.8867924528301887, Precision: 0.8867924528301887\n","K-Fold:  2 Epoch:  75 Train loss:  0.003486377713021024\n","K-Fold: 2, Epoch: 75, Accuracy: 0.8679245283018868, Recall: 0.8679245283018868, F1: 0.8679245283018869, Precision: 0.8679245283018868\n","K-Fold:  2 Epoch:  76 Train loss:  0.0005061950256016903\n","K-Fold: 2, Epoch: 76, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  77 Train loss:  0.004742750810692087\n","K-Fold: 2, Epoch: 77, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  78 Train loss:  0.0071717387025793345\n","K-Fold: 2, Epoch: 78, Accuracy: 0.8490566037735849, Recall: 0.8490566037735849, F1: 0.8490566037735849, Precision: 0.8490566037735849\n","K-Fold:  2 Epoch:  79 Train loss:  0.00113295784467482\n","K-Fold: 2, Epoch: 79, Accuracy: 0.8773584905660378, Recall: 0.8773584905660378, F1: 0.8773584905660378, Precision: 0.8773584905660378\n","K-Fold:  2 Epoch:  80 Train loss:  0.00042811028950901734\n","K-Fold: 2, Epoch: 80, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  81 Train loss:  0.0008544257257848845\n","K-Fold: 2, Epoch: 81, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  82 Train loss:  0.0007907260052369176\n","K-Fold: 2, Epoch: 82, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  83 Train loss:  0.0002507242861611303\n","K-Fold: 2, Epoch: 83, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  84 Train loss:  0.001282882712985156\n","K-Fold: 2, Epoch: 84, Accuracy: 0.8490566037735849, Recall: 0.8490566037735849, F1: 0.8490566037735849, Precision: 0.8490566037735849\n","K-Fold:  2 Epoch:  85 Train loss:  0.0005170947785830192\n","K-Fold: 2, Epoch: 85, Accuracy: 0.8867924528301887, Recall: 0.8867924528301887, F1: 0.8867924528301887, Precision: 0.8867924528301887\n","K-Fold:  2 Epoch:  86 Train loss:  0.0003388107454936419\n","K-Fold: 2, Epoch: 86, Accuracy: 0.8962264150943396, Recall: 0.8962264150943396, F1: 0.8962264150943396, Precision: 0.8962264150943396\n","K-Fold:  2 Epoch:  87 Train loss:  0.0023414768870030195\n","K-Fold: 2, Epoch: 87, Accuracy: 0.8679245283018868, Recall: 0.8679245283018868, F1: 0.8679245283018869, Precision: 0.8679245283018868\n","K-Fold:  2 Epoch:  88 Train loss:  0.0004775136863567501\n","K-Fold: 2, Epoch: 88, Accuracy: 0.8018867924528302, Recall: 0.8018867924528302, F1: 0.8018867924528302, Precision: 0.8018867924528302\n","K-Fold:  2 Epoch:  89 Train loss:  0.002165893243467768\n","K-Fold: 2, Epoch: 89, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  90 Train loss:  0.003580735897620408\n","K-Fold: 2, Epoch: 90, Accuracy: 0.8490566037735849, Recall: 0.8490566037735849, F1: 0.8490566037735849, Precision: 0.8490566037735849\n","K-Fold:  2 Epoch:  91 Train loss:  0.0018453300565722333\n","K-Fold: 2, Epoch: 91, Accuracy: 0.8679245283018868, Recall: 0.8679245283018868, F1: 0.8679245283018869, Precision: 0.8679245283018868\n","K-Fold:  2 Epoch:  92 Train loss:  0.0005474336882928453\n","K-Fold: 2, Epoch: 92, Accuracy: 0.839622641509434, Recall: 0.839622641509434, F1: 0.839622641509434, Precision: 0.839622641509434\n","K-Fold:  2 Epoch:  93 Train loss:  0.000535758524555214\n","K-Fold: 2, Epoch: 93, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  94 Train loss:  0.0002460579164887479\n","K-Fold: 2, Epoch: 94, Accuracy: 0.8584905660377359, Recall: 0.8584905660377359, F1: 0.8584905660377359, Precision: 0.8584905660377359\n","K-Fold:  2 Epoch:  95 Train loss:  0.00031368107115018314\n","K-Fold: 2, Epoch: 95, Accuracy: 0.8679245283018868, Recall: 0.8679245283018868, F1: 0.8679245283018869, Precision: 0.8679245283018868\n","K-Fold:  2 Epoch:  96 Train loss:  0.05361138758884668\n","K-Fold: 2, Epoch: 96, Accuracy: 0.5754716981132075, Recall: 0.5754716981132075, F1: 0.5754716981132075, Precision: 0.5754716981132075\n","K-Fold:  2 Epoch:  97 Train loss:  0.5954495205410889\n","K-Fold: 2, Epoch: 97, Accuracy: 0.5566037735849056, Recall: 0.5566037735849056, F1: 0.5566037735849056, Precision: 0.5566037735849056\n","K-Fold:  2 Epoch:  98 Train loss:  0.1898806421086192\n","K-Fold: 2, Epoch: 98, Accuracy: 0.5660377358490566, Recall: 0.5660377358490566, F1: 0.5660377358490566, Precision: 0.5660377358490566\n","K-Fold:  2 Epoch:  99 Train loss:  0.07002009358257055\n","K-Fold: 2, Epoch: 99, Accuracy: 0.8113207547169812, Recall: 0.8113207547169812, F1: 0.8113207547169812, Precision: 0.8113207547169812\n","K-Fold:  3 Epoch:  0 Train loss:  1.5859565053667342\n","K-Fold: 3, Epoch: 0, Accuracy: 0.2857142857142857, Recall: 0.2857142857142857, F1: 0.2857142857142857, Precision: 0.2857142857142857\n","K-Fold:  3 Epoch:  1 Train loss:  1.4002528616360255\n","K-Fold: 3, Epoch: 1, Accuracy: 0.3904761904761905, Recall: 0.3904761904761905, F1: 0.3904761904761905, Precision: 0.3904761904761905\n","K-Fold:  3 Epoch:  2 Train loss:  1.3067983814648219\n","K-Fold: 3, Epoch: 2, Accuracy: 0.17142857142857143, Recall: 0.17142857142857143, F1: 0.17142857142857143, Precision: 0.17142857142857143\n","K-Fold:  3 Epoch:  3 Train loss:  1.274304015295846\n","K-Fold: 3, Epoch: 3, Accuracy: 0.45714285714285713, Recall: 0.45714285714285713, F1: 0.45714285714285713, Precision: 0.45714285714285713\n","K-Fold:  3 Epoch:  4 Train loss:  1.280768632888794\n","K-Fold: 3, Epoch: 4, Accuracy: 0.41904761904761906, Recall: 0.41904761904761906, F1: 0.41904761904761906, Precision: 0.41904761904761906\n","K-Fold:  3 Epoch:  5 Train loss:  1.223793476819992\n","K-Fold: 3, Epoch: 5, Accuracy: 0.38095238095238093, Recall: 0.38095238095238093, F1: 0.38095238095238093, Precision: 0.38095238095238093\n","K-Fold:  3 Epoch:  6 Train loss:  1.2154187602656228\n","K-Fold: 3, Epoch: 6, Accuracy: 0.4380952380952381, Recall: 0.4380952380952381, F1: 0.4380952380952381, Precision: 0.4380952380952381\n","K-Fold:  3 Epoch:  7 Train loss:  1.1167334829057967\n","K-Fold: 3, Epoch: 7, Accuracy: 0.44761904761904764, Recall: 0.44761904761904764, F1: 0.44761904761904764, Precision: 0.44761904761904764\n","K-Fold:  3 Epoch:  8 Train loss:  1.0663819440773554\n","K-Fold: 3, Epoch: 8, Accuracy: 0.49523809523809526, Recall: 0.49523809523809526, F1: 0.49523809523809526, Precision: 0.49523809523809526\n","K-Fold:  3 Epoch:  9 Train loss:  0.9421102617468152\n","K-Fold: 3, Epoch: 9, Accuracy: 0.4666666666666667, Recall: 0.4666666666666667, F1: 0.4666666666666667, Precision: 0.4666666666666667\n","K-Fold:  3 Epoch:  10 Train loss:  0.9515196297849927\n","K-Fold: 3, Epoch: 10, Accuracy: 0.34285714285714286, Recall: 0.34285714285714286, F1: 0.34285714285714286, Precision: 0.34285714285714286\n","K-Fold:  3 Epoch:  11 Train loss:  0.9057793361800057\n","K-Fold: 3, Epoch: 11, Accuracy: 0.41904761904761906, Recall: 0.41904761904761906, F1: 0.41904761904761906, Precision: 0.41904761904761906\n","K-Fold:  3 Epoch:  12 Train loss:  0.8031415130410876\n","K-Fold: 3, Epoch: 12, Accuracy: 0.4095238095238095, Recall: 0.4095238095238095, F1: 0.4095238095238095, Precision: 0.4095238095238095\n","K-Fold:  3 Epoch:  13 Train loss:  0.8566028135163444\n","K-Fold: 3, Epoch: 13, Accuracy: 0.4666666666666667, Recall: 0.4666666666666667, F1: 0.4666666666666667, Precision: 0.4666666666666667\n","K-Fold:  3 Epoch:  14 Train loss:  0.7817326358386448\n","K-Fold: 3, Epoch: 14, Accuracy: 0.44761904761904764, Recall: 0.44761904761904764, F1: 0.44761904761904764, Precision: 0.44761904761904764\n","K-Fold:  3 Epoch:  15 Train loss:  0.7035636178084782\n","K-Fold: 3, Epoch: 15, Accuracy: 0.42857142857142855, Recall: 0.42857142857142855, F1: 0.42857142857142855, Precision: 0.42857142857142855\n","K-Fold:  3 Epoch:  16 Train loss:  0.5544503182172775\n","K-Fold: 3, Epoch: 16, Accuracy: 0.5904761904761905, Recall: 0.5904761904761905, F1: 0.5904761904761905, Precision: 0.5904761904761905\n","K-Fold:  3 Epoch:  17 Train loss:  0.3603767848440579\n","K-Fold: 3, Epoch: 17, Accuracy: 0.4380952380952381, Recall: 0.4380952380952381, F1: 0.4380952380952381, Precision: 0.4380952380952381\n","K-Fold:  3 Epoch:  18 Train loss:  0.3321204994406019\n","K-Fold: 3, Epoch: 18, Accuracy: 0.5333333333333333, Recall: 0.5333333333333333, F1: 0.5333333333333333, Precision: 0.5333333333333333\n","K-Fold:  3 Epoch:  19 Train loss:  0.5208313124520438\n","K-Fold: 3, Epoch: 19, Accuracy: 0.4666666666666667, Recall: 0.4666666666666667, F1: 0.4666666666666667, Precision: 0.4666666666666667\n","K-Fold:  3 Epoch:  20 Train loss:  0.40189819570098606\n","K-Fold: 3, Epoch: 20, Accuracy: 0.6095238095238096, Recall: 0.6095238095238096, F1: 0.6095238095238096, Precision: 0.6095238095238096\n","K-Fold:  3 Epoch:  21 Train loss:  0.518598735332489\n","K-Fold: 3, Epoch: 21, Accuracy: 0.6476190476190476, Recall: 0.6476190476190476, F1: 0.6476190476190476, Precision: 0.6476190476190476\n","K-Fold:  3 Epoch:  22 Train loss:  0.34595829035554615\n","K-Fold: 3, Epoch: 22, Accuracy: 0.6285714285714286, Recall: 0.6285714285714286, F1: 0.6285714285714286, Precision: 0.6285714285714286\n","K-Fold:  3 Epoch:  23 Train loss:  0.2380493368421282\n","K-Fold: 3, Epoch: 23, Accuracy: 0.7047619047619048, Recall: 0.7047619047619048, F1: 0.7047619047619048, Precision: 0.7047619047619048\n","K-Fold:  3 Epoch:  24 Train loss:  0.21503645394529616\n","K-Fold: 3, Epoch: 24, Accuracy: 0.45714285714285713, Recall: 0.45714285714285713, F1: 0.45714285714285713, Precision: 0.45714285714285713\n","K-Fold:  3 Epoch:  25 Train loss:  0.16980764203305757\n","K-Fold: 3, Epoch: 25, Accuracy: 0.6952380952380952, Recall: 0.6952380952380952, F1: 0.6952380952380952, Precision: 0.6952380952380952\n","K-Fold:  3 Epoch:  26 Train loss:  0.16884561169094273\n","K-Fold: 3, Epoch: 26, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  3 Epoch:  27 Train loss:  0.07884519254522664\n","K-Fold: 3, Epoch: 27, Accuracy: 0.6761904761904762, Recall: 0.6761904761904762, F1: 0.6761904761904762, Precision: 0.6761904761904762\n","K-Fold:  3 Epoch:  28 Train loss:  0.10498102654569916\n","K-Fold: 3, Epoch: 28, Accuracy: 0.6666666666666666, Recall: 0.6666666666666666, F1: 0.6666666666666666, Precision: 0.6666666666666666\n","K-Fold:  3 Epoch:  29 Train loss:  0.3714514560997486\n","K-Fold: 3, Epoch: 29, Accuracy: 0.29523809523809524, Recall: 0.29523809523809524, F1: 0.29523809523809524, Precision: 0.29523809523809524\n","K-Fold:  3 Epoch:  30 Train loss:  0.3313368914116706\n","K-Fold: 3, Epoch: 30, Accuracy: 0.7047619047619048, Recall: 0.7047619047619048, F1: 0.7047619047619048, Precision: 0.7047619047619048\n","K-Fold:  3 Epoch:  31 Train loss:  0.24401393852063588\n","K-Fold: 3, Epoch: 31, Accuracy: 0.780952380952381, Recall: 0.780952380952381, F1: 0.780952380952381, Precision: 0.780952380952381\n","K-Fold:  3 Epoch:  32 Train loss:  0.08431947816695486\n","K-Fold: 3, Epoch: 32, Accuracy: 0.6952380952380952, Recall: 0.6952380952380952, F1: 0.6952380952380952, Precision: 0.6952380952380952\n","K-Fold:  3 Epoch:  33 Train loss:  0.05400668493738132\n","K-Fold: 3, Epoch: 33, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  3 Epoch:  34 Train loss:  0.048528247113738744\n","K-Fold: 3, Epoch: 34, Accuracy: 0.5619047619047619, Recall: 0.5619047619047619, F1: 0.5619047619047619, Precision: 0.5619047619047619\n","K-Fold:  3 Epoch:  35 Train loss:  0.06424734338984958\n","K-Fold: 3, Epoch: 35, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  3 Epoch:  36 Train loss:  0.010220787986846907\n","K-Fold: 3, Epoch: 36, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  3 Epoch:  37 Train loss:  0.007118262017944029\n","K-Fold: 3, Epoch: 37, Accuracy: 0.8952380952380953, Recall: 0.8952380952380953, F1: 0.8952380952380953, Precision: 0.8952380952380953\n","K-Fold:  3 Epoch:  38 Train loss:  0.0069090655306354165\n","K-Fold: 3, Epoch: 38, Accuracy: 0.8857142857142857, Recall: 0.8857142857142857, F1: 0.8857142857142857, Precision: 0.8857142857142857\n","K-Fold:  3 Epoch:  39 Train loss:  0.007928451011788898\n","K-Fold: 3, Epoch: 39, Accuracy: 0.8952380952380953, Recall: 0.8952380952380953, F1: 0.8952380952380953, Precision: 0.8952380952380953\n","K-Fold:  3 Epoch:  40 Train loss:  0.011667874687451072\n","K-Fold: 3, Epoch: 40, Accuracy: 0.9142857142857143, Recall: 0.9142857142857143, F1: 0.9142857142857143, Precision: 0.9142857142857143\n","K-Fold:  3 Epoch:  41 Train loss:  0.014242801648963774\n","K-Fold: 3, Epoch: 41, Accuracy: 0.7904761904761904, Recall: 0.7904761904761904, F1: 0.7904761904761904, Precision: 0.7904761904761904\n","K-Fold:  3 Epoch:  42 Train loss:  0.01172888109327427\n","K-Fold: 3, Epoch: 42, Accuracy: 0.8, Recall: 0.8, F1: 0.8000000000000002, Precision: 0.8\n","K-Fold:  3 Epoch:  43 Train loss:  0.02247063645959965\n","K-Fold: 3, Epoch: 43, Accuracy: 0.6952380952380952, Recall: 0.6952380952380952, F1: 0.6952380952380952, Precision: 0.6952380952380952\n","K-Fold:  3 Epoch:  44 Train loss:  0.03602461375079916\n","K-Fold: 3, Epoch: 44, Accuracy: 0.6285714285714286, Recall: 0.6285714285714286, F1: 0.6285714285714286, Precision: 0.6285714285714286\n","K-Fold:  3 Epoch:  45 Train loss:  0.08973863370816357\n","K-Fold: 3, Epoch: 45, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  3 Epoch:  46 Train loss:  0.11984342552854546\n","K-Fold: 3, Epoch: 46, Accuracy: 0.7333333333333333, Recall: 0.7333333333333333, F1: 0.7333333333333333, Precision: 0.7333333333333333\n","K-Fold:  3 Epoch:  47 Train loss:  0.09601503321235734\n","K-Fold: 3, Epoch: 47, Accuracy: 0.5714285714285714, Recall: 0.5714285714285714, F1: 0.5714285714285714, Precision: 0.5714285714285714\n","K-Fold:  3 Epoch:  48 Train loss:  0.02500556514132768\n","K-Fold: 3, Epoch: 48, Accuracy: 0.7238095238095238, Recall: 0.7238095238095238, F1: 0.7238095238095238, Precision: 0.7238095238095238\n","K-Fold:  3 Epoch:  49 Train loss:  0.01308561091510845\n","K-Fold: 3, Epoch: 49, Accuracy: 0.819047619047619, Recall: 0.819047619047619, F1: 0.819047619047619, Precision: 0.819047619047619\n","K-Fold:  3 Epoch:  50 Train loss:  0.02283876684045286\n","K-Fold: 3, Epoch: 50, Accuracy: 0.8, Recall: 0.8, F1: 0.8000000000000002, Precision: 0.8\n","K-Fold:  3 Epoch:  51 Train loss:  0.04234244568007333\n","K-Fold: 3, Epoch: 51, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  3 Epoch:  52 Train loss:  0.1734242181659543\n","K-Fold: 3, Epoch: 52, Accuracy: 0.6476190476190476, Recall: 0.6476190476190476, F1: 0.6476190476190476, Precision: 0.6476190476190476\n","K-Fold:  3 Epoch:  53 Train loss:  0.18096561197723662\n","K-Fold: 3, Epoch: 53, Accuracy: 0.7904761904761904, Recall: 0.7904761904761904, F1: 0.7904761904761904, Precision: 0.7904761904761904\n","K-Fold:  3 Epoch:  54 Train loss:  0.07046606135554612\n","K-Fold: 3, Epoch: 54, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  3 Epoch:  55 Train loss:  0.12910028919577599\n","K-Fold: 3, Epoch: 55, Accuracy: 0.5714285714285714, Recall: 0.5714285714285714, F1: 0.5714285714285714, Precision: 0.5714285714285714\n","K-Fold:  3 Epoch:  56 Train loss:  0.0573422413685226\n","K-Fold: 3, Epoch: 56, Accuracy: 0.7047619047619048, Recall: 0.7047619047619048, F1: 0.7047619047619048, Precision: 0.7047619047619048\n","K-Fold:  3 Epoch:  57 Train loss:  0.08710755687206984\n","K-Fold: 3, Epoch: 57, Accuracy: 0.580952380952381, Recall: 0.580952380952381, F1: 0.580952380952381, Precision: 0.580952380952381\n","K-Fold:  3 Epoch:  58 Train loss:  0.09654597041662782\n","K-Fold: 3, Epoch: 58, Accuracy: 0.7428571428571429, Recall: 0.7428571428571429, F1: 0.7428571428571429, Precision: 0.7428571428571429\n","K-Fold:  3 Epoch:  59 Train loss:  0.05827102417658482\n","K-Fold: 3, Epoch: 59, Accuracy: 0.6, Recall: 0.6, F1: 0.6, Precision: 0.6\n","K-Fold:  3 Epoch:  60 Train loss:  0.026090099867100695\n","K-Fold: 3, Epoch: 60, Accuracy: 0.7523809523809524, Recall: 0.7523809523809524, F1: 0.7523809523809524, Precision: 0.7523809523809524\n","K-Fold:  3 Epoch:  61 Train loss:  0.01275626993239192\n","K-Fold: 3, Epoch: 61, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  3 Epoch:  62 Train loss:  0.0044988971827219105\n","K-Fold: 3, Epoch: 62, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  3 Epoch:  63 Train loss:  0.0031990733820878503\n","K-Fold: 3, Epoch: 63, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  3 Epoch:  64 Train loss:  0.0058536160857849085\n","K-Fold: 3, Epoch: 64, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  65 Train loss:  0.002404092499221276\n","K-Fold: 3, Epoch: 65, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  66 Train loss:  0.001645133990678005\n","K-Fold: 3, Epoch: 66, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  67 Train loss:  0.0013170397838361428\n","K-Fold: 3, Epoch: 67, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  68 Train loss:  0.000582857989814199\n","K-Fold: 3, Epoch: 68, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  69 Train loss:  0.000753882438792581\n","K-Fold: 3, Epoch: 69, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  70 Train loss:  0.00043602949153864756\n","K-Fold: 3, Epoch: 70, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  3 Epoch:  71 Train loss:  0.000478046339204801\n","K-Fold: 3, Epoch: 71, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  72 Train loss:  0.0003173939777687857\n","K-Fold: 3, Epoch: 72, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  73 Train loss:  0.0003618035843828693\n","K-Fold: 3, Epoch: 73, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  74 Train loss:  0.0005745595689014797\n","K-Fold: 3, Epoch: 74, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  75 Train loss:  0.0002675971533920217\n","K-Fold: 3, Epoch: 75, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  76 Train loss:  0.0003517917653620576\n","K-Fold: 3, Epoch: 76, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  77 Train loss:  0.0002314065937493329\n","K-Fold: 3, Epoch: 77, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  3 Epoch:  78 Train loss:  0.0007618430911731723\n","K-Fold: 3, Epoch: 78, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  3 Epoch:  79 Train loss:  0.00030086352178061915\n","K-Fold: 3, Epoch: 79, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  3 Epoch:  80 Train loss:  0.0002144870418955439\n","K-Fold: 3, Epoch: 80, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  3 Epoch:  81 Train loss:  0.006887994179643491\n","K-Fold: 3, Epoch: 81, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  3 Epoch:  82 Train loss:  0.04408291059891261\n","K-Fold: 3, Epoch: 82, Accuracy: 0.7904761904761904, Recall: 0.7904761904761904, F1: 0.7904761904761904, Precision: 0.7904761904761904\n","K-Fold:  3 Epoch:  83 Train loss:  0.0244288743935093\n","K-Fold: 3, Epoch: 83, Accuracy: 0.7428571428571429, Recall: 0.7428571428571429, F1: 0.7428571428571429, Precision: 0.7428571428571429\n","K-Fold:  3 Epoch:  84 Train loss:  0.16892719659621694\n","K-Fold: 3, Epoch: 84, Accuracy: 0.4666666666666667, Recall: 0.4666666666666667, F1: 0.4666666666666667, Precision: 0.4666666666666667\n","K-Fold:  3 Epoch:  85 Train loss:  0.21268371718802623\n","K-Fold: 3, Epoch: 85, Accuracy: 0.6857142857142857, Recall: 0.6857142857142857, F1: 0.6857142857142857, Precision: 0.6857142857142857\n","K-Fold:  3 Epoch:  86 Train loss:  0.2150230291299522\n","K-Fold: 3, Epoch: 86, Accuracy: 0.7333333333333333, Recall: 0.7333333333333333, F1: 0.7333333333333333, Precision: 0.7333333333333333\n","K-Fold:  3 Epoch:  87 Train loss:  0.13727869163267314\n","K-Fold: 3, Epoch: 87, Accuracy: 0.580952380952381, Recall: 0.580952380952381, F1: 0.580952380952381, Precision: 0.580952380952381\n","K-Fold:  3 Epoch:  88 Train loss:  0.15019373389493143\n","K-Fold: 3, Epoch: 88, Accuracy: 0.8, Recall: 0.8, F1: 0.8000000000000002, Precision: 0.8\n","K-Fold:  3 Epoch:  89 Train loss:  0.059256618143990636\n","K-Fold: 3, Epoch: 89, Accuracy: 0.6857142857142857, Recall: 0.6857142857142857, F1: 0.6857142857142857, Precision: 0.6857142857142857\n","K-Fold:  3 Epoch:  90 Train loss:  0.0531971626332961\n","K-Fold: 3, Epoch: 90, Accuracy: 0.7714285714285715, Recall: 0.7714285714285715, F1: 0.7714285714285715, Precision: 0.7714285714285715\n","K-Fold:  3 Epoch:  91 Train loss:  0.022130042006860355\n","K-Fold: 3, Epoch: 91, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  3 Epoch:  92 Train loss:  0.014857613078285275\n","K-Fold: 3, Epoch: 92, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  3 Epoch:  93 Train loss:  0.0030876021171154988\n","K-Fold: 3, Epoch: 93, Accuracy: 0.7238095238095238, Recall: 0.7238095238095238, F1: 0.7238095238095238, Precision: 0.7238095238095238\n","K-Fold:  3 Epoch:  94 Train loss:  0.003896887106488326\n","K-Fold: 3, Epoch: 94, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  3 Epoch:  95 Train loss:  0.006515845456825835\n","K-Fold: 3, Epoch: 95, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  3 Epoch:  96 Train loss:  0.023874167090980336\n","K-Fold: 3, Epoch: 96, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  3 Epoch:  97 Train loss:  0.16016316827153787\n","K-Fold: 3, Epoch: 97, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  3 Epoch:  98 Train loss:  0.3289527083480997\n","K-Fold: 3, Epoch: 98, Accuracy: 0.37142857142857144, Recall: 0.37142857142857144, F1: 0.37142857142857144, Precision: 0.37142857142857144\n","K-Fold:  3 Epoch:  99 Train loss:  0.26894065365195274\n","K-Fold: 3, Epoch: 99, Accuracy: 0.6666666666666666, Recall: 0.6666666666666666, F1: 0.6666666666666666, Precision: 0.6666666666666666\n","K-Fold:  4 Epoch:  0 Train loss:  1.5382380144936698\n","K-Fold: 4, Epoch: 0, Accuracy: 0.3142857142857143, Recall: 0.3142857142857143, F1: 0.3142857142857143, Precision: 0.3142857142857143\n","K-Fold:  4 Epoch:  1 Train loss:  1.4049270408494132\n","K-Fold: 4, Epoch: 1, Accuracy: 0.3904761904761905, Recall: 0.3904761904761905, F1: 0.3904761904761905, Precision: 0.3904761904761905\n","K-Fold:  4 Epoch:  2 Train loss:  1.238371193408966\n","K-Fold: 4, Epoch: 2, Accuracy: 0.4666666666666667, Recall: 0.4666666666666667, F1: 0.4666666666666667, Precision: 0.4666666666666667\n","K-Fold:  4 Epoch:  3 Train loss:  1.1118701015199934\n","K-Fold: 4, Epoch: 3, Accuracy: 0.6190476190476191, Recall: 0.6190476190476191, F1: 0.6190476190476191, Precision: 0.6190476190476191\n","K-Fold:  4 Epoch:  4 Train loss:  1.021818995475769\n","K-Fold: 4, Epoch: 4, Accuracy: 0.3142857142857143, Recall: 0.3142857142857143, F1: 0.3142857142857143, Precision: 0.3142857142857143\n","K-Fold:  4 Epoch:  5 Train loss:  0.9343338821615491\n","K-Fold: 4, Epoch: 5, Accuracy: 0.47619047619047616, Recall: 0.47619047619047616, F1: 0.47619047619047616, Precision: 0.47619047619047616\n","K-Fold:  4 Epoch:  6 Train loss:  0.9599118530750275\n","K-Fold: 4, Epoch: 6, Accuracy: 0.6095238095238096, Recall: 0.6095238095238096, F1: 0.6095238095238096, Precision: 0.6095238095238096\n","K-Fold:  4 Epoch:  7 Train loss:  0.7541330116135734\n","K-Fold: 4, Epoch: 7, Accuracy: 0.6666666666666666, Recall: 0.6666666666666666, F1: 0.6666666666666666, Precision: 0.6666666666666666\n","K-Fold:  4 Epoch:  8 Train loss:  0.6438581134591784\n","K-Fold: 4, Epoch: 8, Accuracy: 0.6761904761904762, Recall: 0.6761904761904762, F1: 0.6761904761904762, Precision: 0.6761904761904762\n","K-Fold:  4 Epoch:  9 Train loss:  0.5985580065420696\n","K-Fold: 4, Epoch: 9, Accuracy: 0.4380952380952381, Recall: 0.4380952380952381, F1: 0.4380952380952381, Precision: 0.4380952380952381\n","K-Fold:  4 Epoch:  10 Train loss:  0.5374201355235917\n","K-Fold: 4, Epoch: 10, Accuracy: 0.3904761904761905, Recall: 0.3904761904761905, F1: 0.3904761904761905, Precision: 0.3904761904761905\n","K-Fold:  4 Epoch:  11 Train loss:  0.6397123336791992\n","K-Fold: 4, Epoch: 11, Accuracy: 0.7047619047619048, Recall: 0.7047619047619048, F1: 0.7047619047619048, Precision: 0.7047619047619048\n","K-Fold:  4 Epoch:  12 Train loss:  0.5649664614881788\n","K-Fold: 4, Epoch: 12, Accuracy: 0.38095238095238093, Recall: 0.38095238095238093, F1: 0.38095238095238093, Precision: 0.38095238095238093\n","K-Fold:  4 Epoch:  13 Train loss:  0.4416005164384842\n","K-Fold: 4, Epoch: 13, Accuracy: 0.6857142857142857, Recall: 0.6857142857142857, F1: 0.6857142857142857, Precision: 0.6857142857142857\n","K-Fold:  4 Epoch:  14 Train loss:  0.48233235095228466\n","K-Fold: 4, Epoch: 14, Accuracy: 0.6190476190476191, Recall: 0.6190476190476191, F1: 0.6190476190476191, Precision: 0.6190476190476191\n","K-Fold:  4 Epoch:  15 Train loss:  0.4007280692458153\n","K-Fold: 4, Epoch: 15, Accuracy: 0.3238095238095238, Recall: 0.3238095238095238, F1: 0.3238095238095238, Precision: 0.3238095238095238\n","K-Fold:  4 Epoch:  16 Train loss:  0.4740910732320377\n","K-Fold: 4, Epoch: 16, Accuracy: 0.7619047619047619, Recall: 0.7619047619047619, F1: 0.7619047619047619, Precision: 0.7619047619047619\n","K-Fold:  4 Epoch:  17 Train loss:  0.3241440217409815\n","K-Fold: 4, Epoch: 17, Accuracy: 0.7333333333333333, Recall: 0.7333333333333333, F1: 0.7333333333333333, Precision: 0.7333333333333333\n","K-Fold:  4 Epoch:  18 Train loss:  0.3686166416321482\n","K-Fold: 4, Epoch: 18, Accuracy: 0.7142857142857143, Recall: 0.7142857142857143, F1: 0.7142857142857143, Precision: 0.7142857142857143\n","K-Fold:  4 Epoch:  19 Train loss:  0.5386834133948598\n","K-Fold: 4, Epoch: 19, Accuracy: 0.6571428571428571, Recall: 0.6571428571428571, F1: 0.6571428571428571, Precision: 0.6571428571428571\n","K-Fold:  4 Epoch:  20 Train loss:  0.3896464301007135\n","K-Fold: 4, Epoch: 20, Accuracy: 0.3904761904761905, Recall: 0.3904761904761905, F1: 0.3904761904761905, Precision: 0.3904761904761905\n","K-Fold:  4 Epoch:  21 Train loss:  0.2419172115623951\n","K-Fold: 4, Epoch: 21, Accuracy: 0.6476190476190476, Recall: 0.6476190476190476, F1: 0.6476190476190476, Precision: 0.6476190476190476\n","K-Fold:  4 Epoch:  22 Train loss:  0.17723935203892843\n","K-Fold: 4, Epoch: 22, Accuracy: 0.819047619047619, Recall: 0.819047619047619, F1: 0.819047619047619, Precision: 0.819047619047619\n","K-Fold:  4 Epoch:  23 Train loss:  0.16727364542228834\n","K-Fold: 4, Epoch: 23, Accuracy: 0.580952380952381, Recall: 0.580952380952381, F1: 0.580952380952381, Precision: 0.580952380952381\n","K-Fold:  4 Epoch:  24 Train loss:  0.18736133032611438\n","K-Fold: 4, Epoch: 24, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  25 Train loss:  0.3790931744234903\n","K-Fold: 4, Epoch: 25, Accuracy: 0.6857142857142857, Recall: 0.6857142857142857, F1: 0.6857142857142857, Precision: 0.6857142857142857\n","K-Fold:  4 Epoch:  26 Train loss:  0.3697905237121241\n","K-Fold: 4, Epoch: 26, Accuracy: 0.5047619047619047, Recall: 0.5047619047619047, F1: 0.5047619047619047, Precision: 0.5047619047619047\n","K-Fold:  4 Epoch:  27 Train loss:  0.6130589619278908\n","K-Fold: 4, Epoch: 27, Accuracy: 0.3619047619047619, Recall: 0.3619047619047619, F1: 0.3619047619047619, Precision: 0.3619047619047619\n","K-Fold:  4 Epoch:  28 Train loss:  0.24399228074720927\n","K-Fold: 4, Epoch: 28, Accuracy: 0.6476190476190476, Recall: 0.6476190476190476, F1: 0.6476190476190476, Precision: 0.6476190476190476\n","K-Fold:  4 Epoch:  29 Train loss:  0.14566743852836744\n","K-Fold: 4, Epoch: 29, Accuracy: 0.7428571428571429, Recall: 0.7428571428571429, F1: 0.7428571428571429, Precision: 0.7428571428571429\n","K-Fold:  4 Epoch:  30 Train loss:  0.11489647121301719\n","K-Fold: 4, Epoch: 30, Accuracy: 0.7523809523809524, Recall: 0.7523809523809524, F1: 0.7523809523809524, Precision: 0.7523809523809524\n","K-Fold:  4 Epoch:  31 Train loss:  0.24719732720404863\n","K-Fold: 4, Epoch: 31, Accuracy: 0.49523809523809526, Recall: 0.49523809523809526, F1: 0.49523809523809526, Precision: 0.49523809523809526\n","K-Fold:  4 Epoch:  32 Train loss:  0.1480996725814683\n","K-Fold: 4, Epoch: 32, Accuracy: 0.6761904761904762, Recall: 0.6761904761904762, F1: 0.6761904761904762, Precision: 0.6761904761904762\n","K-Fold:  4 Epoch:  33 Train loss:  0.10429942088999919\n","K-Fold: 4, Epoch: 33, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  4 Epoch:  34 Train loss:  0.07279087789356709\n","K-Fold: 4, Epoch: 34, Accuracy: 0.7238095238095238, Recall: 0.7238095238095238, F1: 0.7238095238095238, Precision: 0.7238095238095238\n","K-Fold:  4 Epoch:  35 Train loss:  0.061350478258516104\n","K-Fold: 4, Epoch: 35, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  36 Train loss:  0.08104193599761597\n","K-Fold: 4, Epoch: 36, Accuracy: 0.5047619047619047, Recall: 0.5047619047619047, F1: 0.5047619047619047, Precision: 0.5047619047619047\n","K-Fold:  4 Epoch:  37 Train loss:  0.1038672573797937\n","K-Fold: 4, Epoch: 37, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  38 Train loss:  0.04803706432825753\n","K-Fold: 4, Epoch: 38, Accuracy: 0.6571428571428571, Recall: 0.6571428571428571, F1: 0.6571428571428571, Precision: 0.6571428571428571\n","K-Fold:  4 Epoch:  39 Train loss:  0.10595705306955747\n","K-Fold: 4, Epoch: 39, Accuracy: 0.5428571428571428, Recall: 0.5428571428571428, F1: 0.5428571428571428, Precision: 0.5428571428571428\n","K-Fold:  4 Epoch:  40 Train loss:  0.2087124673915761\n","K-Fold: 4, Epoch: 40, Accuracy: 0.6285714285714286, Recall: 0.6285714285714286, F1: 0.6285714285714286, Precision: 0.6285714285714286\n","K-Fold:  4 Epoch:  41 Train loss:  0.07690335737009134\n","K-Fold: 4, Epoch: 41, Accuracy: 0.780952380952381, Recall: 0.780952380952381, F1: 0.780952380952381, Precision: 0.780952380952381\n","K-Fold:  4 Epoch:  42 Train loss:  0.04698400821403733\n","K-Fold: 4, Epoch: 42, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  4 Epoch:  43 Train loss:  0.29925965604239274\n","K-Fold: 4, Epoch: 43, Accuracy: 0.8, Recall: 0.8, F1: 0.8000000000000002, Precision: 0.8\n","K-Fold:  4 Epoch:  44 Train loss:  0.7667100990989378\n","K-Fold: 4, Epoch: 44, Accuracy: 0.37142857142857144, Recall: 0.37142857142857144, F1: 0.37142857142857144, Precision: 0.37142857142857144\n","K-Fold:  4 Epoch:  45 Train loss:  0.3929800231541906\n","K-Fold: 4, Epoch: 45, Accuracy: 0.5333333333333333, Recall: 0.5333333333333333, F1: 0.5333333333333333, Precision: 0.5333333333333333\n","K-Fold:  4 Epoch:  46 Train loss:  0.25070637251649586\n","K-Fold: 4, Epoch: 46, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  4 Epoch:  47 Train loss:  0.14150728657841682\n","K-Fold: 4, Epoch: 47, Accuracy: 0.6952380952380952, Recall: 0.6952380952380952, F1: 0.6952380952380952, Precision: 0.6952380952380952\n","K-Fold:  4 Epoch:  48 Train loss:  0.06657080977622952\n","K-Fold: 4, Epoch: 48, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  49 Train loss:  0.04255645374567913\n","K-Fold: 4, Epoch: 49, Accuracy: 0.8476190476190476, Recall: 0.8476190476190476, F1: 0.8476190476190476, Precision: 0.8476190476190476\n","K-Fold:  4 Epoch:  50 Train loss:  0.023413347080349922\n","K-Fold: 4, Epoch: 50, Accuracy: 0.819047619047619, Recall: 0.819047619047619, F1: 0.819047619047619, Precision: 0.819047619047619\n","K-Fold:  4 Epoch:  51 Train loss:  0.02263126563879528\n","K-Fold: 4, Epoch: 51, Accuracy: 0.7523809523809524, Recall: 0.7523809523809524, F1: 0.7523809523809524, Precision: 0.7523809523809524\n","K-Fold:  4 Epoch:  52 Train loss:  0.04431796632707119\n","K-Fold: 4, Epoch: 52, Accuracy: 0.6761904761904762, Recall: 0.6761904761904762, F1: 0.6761904761904762, Precision: 0.6761904761904762\n","K-Fold:  4 Epoch:  53 Train loss:  0.041279707137229185\n","K-Fold: 4, Epoch: 53, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  54 Train loss:  0.08911754284054041\n","K-Fold: 4, Epoch: 54, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  4 Epoch:  55 Train loss:  0.21966280954490816\n","K-Fold: 4, Epoch: 55, Accuracy: 0.3523809523809524, Recall: 0.3523809523809524, F1: 0.3523809523809524, Precision: 0.3523809523809524\n","K-Fold:  4 Epoch:  56 Train loss:  0.27977072899895056\n","K-Fold: 4, Epoch: 56, Accuracy: 0.5714285714285714, Recall: 0.5714285714285714, F1: 0.5714285714285714, Precision: 0.5714285714285714\n","K-Fold:  4 Epoch:  57 Train loss:  0.1528637276164123\n","K-Fold: 4, Epoch: 57, Accuracy: 0.7619047619047619, Recall: 0.7619047619047619, F1: 0.7619047619047619, Precision: 0.7619047619047619\n","K-Fold:  4 Epoch:  58 Train loss:  0.026620547220643078\n","K-Fold: 4, Epoch: 58, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  4 Epoch:  59 Train loss:  0.01169254956766963\n","K-Fold: 4, Epoch: 59, Accuracy: 0.8857142857142857, Recall: 0.8857142857142857, F1: 0.8857142857142857, Precision: 0.8857142857142857\n","K-Fold:  4 Epoch:  60 Train loss:  0.008401768282055855\n","K-Fold: 4, Epoch: 60, Accuracy: 0.8952380952380953, Recall: 0.8952380952380953, F1: 0.8952380952380953, Precision: 0.8952380952380953\n","K-Fold:  4 Epoch:  61 Train loss:  0.005068490198547286\n","K-Fold: 4, Epoch: 61, Accuracy: 0.8952380952380953, Recall: 0.8952380952380953, F1: 0.8952380952380953, Precision: 0.8952380952380953\n","K-Fold:  4 Epoch:  62 Train loss:  0.004947166831698269\n","K-Fold: 4, Epoch: 62, Accuracy: 0.8857142857142857, Recall: 0.8857142857142857, F1: 0.8857142857142857, Precision: 0.8857142857142857\n","K-Fold:  4 Epoch:  63 Train loss:  0.0021124939791791674\n","K-Fold: 4, Epoch: 63, Accuracy: 0.8476190476190476, Recall: 0.8476190476190476, F1: 0.8476190476190476, Precision: 0.8476190476190476\n","K-Fold:  4 Epoch:  64 Train loss:  0.04451037751277909\n","K-Fold: 4, Epoch: 64, Accuracy: 0.7714285714285715, Recall: 0.7714285714285715, F1: 0.7714285714285715, Precision: 0.7714285714285715\n","K-Fold:  4 Epoch:  65 Train loss:  0.23228701736245835\n","K-Fold: 4, Epoch: 65, Accuracy: 0.6476190476190476, Recall: 0.6476190476190476, F1: 0.6476190476190476, Precision: 0.6476190476190476\n","K-Fold:  4 Epoch:  66 Train loss:  0.19003761736010866\n","K-Fold: 4, Epoch: 66, Accuracy: 0.6952380952380952, Recall: 0.6952380952380952, F1: 0.6952380952380952, Precision: 0.6952380952380952\n","K-Fold:  4 Epoch:  67 Train loss:  0.06991742933834237\n","K-Fold: 4, Epoch: 67, Accuracy: 0.780952380952381, Recall: 0.780952380952381, F1: 0.780952380952381, Precision: 0.780952380952381\n","K-Fold:  4 Epoch:  68 Train loss:  0.08453111540127013\n","K-Fold: 4, Epoch: 68, Accuracy: 0.7428571428571429, Recall: 0.7428571428571429, F1: 0.7428571428571429, Precision: 0.7428571428571429\n","K-Fold:  4 Epoch:  69 Train loss:  0.11372203724126198\n","K-Fold: 4, Epoch: 69, Accuracy: 0.6666666666666666, Recall: 0.6666666666666666, F1: 0.6666666666666666, Precision: 0.6666666666666666\n","K-Fold:  4 Epoch:  70 Train loss:  0.1461340019678963\n","K-Fold: 4, Epoch: 70, Accuracy: 0.6, Recall: 0.6, F1: 0.6, Precision: 0.6\n","K-Fold:  4 Epoch:  71 Train loss:  0.22887599616244966\n","K-Fold: 4, Epoch: 71, Accuracy: 0.45714285714285713, Recall: 0.45714285714285713, F1: 0.45714285714285713, Precision: 0.45714285714285713\n","K-Fold:  4 Epoch:  72 Train loss:  0.08422937324004513\n","K-Fold: 4, Epoch: 72, Accuracy: 0.44761904761904764, Recall: 0.44761904761904764, F1: 0.44761904761904764, Precision: 0.44761904761904764\n","K-Fold:  4 Epoch:  73 Train loss:  0.056472520011344125\n","K-Fold: 4, Epoch: 73, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  4 Epoch:  74 Train loss:  0.022344027845455066\n","K-Fold: 4, Epoch: 74, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  75 Train loss:  0.014844767982140183\n","K-Fold: 4, Epoch: 75, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  4 Epoch:  76 Train loss:  0.007880616392607667\n","K-Fold: 4, Epoch: 76, Accuracy: 0.8476190476190476, Recall: 0.8476190476190476, F1: 0.8476190476190476, Precision: 0.8476190476190476\n","K-Fold:  4 Epoch:  77 Train loss:  0.13600829066958145\n","K-Fold: 4, Epoch: 77, Accuracy: 0.8380952380952381, Recall: 0.8380952380952381, F1: 0.8380952380952381, Precision: 0.8380952380952381\n","K-Fold:  4 Epoch:  78 Train loss:  0.30890683909612043\n","K-Fold: 4, Epoch: 78, Accuracy: 0.44761904761904764, Recall: 0.44761904761904764, F1: 0.44761904761904764, Precision: 0.44761904761904764\n","K-Fold:  4 Epoch:  79 Train loss:  0.09058323276362248\n","K-Fold: 4, Epoch: 79, Accuracy: 0.7904761904761904, Recall: 0.7904761904761904, F1: 0.7904761904761904, Precision: 0.7904761904761904\n","K-Fold:  4 Epoch:  80 Train loss:  0.04543773497321776\n","K-Fold: 4, Epoch: 80, Accuracy: 0.7904761904761904, Recall: 0.7904761904761904, F1: 0.7904761904761904, Precision: 0.7904761904761904\n","K-Fold:  4 Epoch:  81 Train loss:  0.08129486381741506\n","K-Fold: 4, Epoch: 81, Accuracy: 0.780952380952381, Recall: 0.780952380952381, F1: 0.780952380952381, Precision: 0.780952380952381\n","K-Fold:  4 Epoch:  82 Train loss:  0.14803427560920163\n","K-Fold: 4, Epoch: 82, Accuracy: 0.7047619047619048, Recall: 0.7047619047619048, F1: 0.7047619047619048, Precision: 0.7047619047619048\n","K-Fold:  4 Epoch:  83 Train loss:  0.12332304138025003\n","K-Fold: 4, Epoch: 83, Accuracy: 0.6285714285714286, Recall: 0.6285714285714286, F1: 0.6285714285714286, Precision: 0.6285714285714286\n","K-Fold:  4 Epoch:  84 Train loss:  0.15844600867213948\n","K-Fold: 4, Epoch: 84, Accuracy: 0.7428571428571429, Recall: 0.7428571428571429, F1: 0.7428571428571429, Precision: 0.7428571428571429\n","K-Fold:  4 Epoch:  85 Train loss:  0.10298298039872732\n","K-Fold: 4, Epoch: 85, Accuracy: 0.5523809523809524, Recall: 0.5523809523809524, F1: 0.5523809523809524, Precision: 0.5523809523809524\n","K-Fold:  4 Epoch:  86 Train loss:  0.04130553181416222\n","K-Fold: 4, Epoch: 86, Accuracy: 0.8095238095238095, Recall: 0.8095238095238095, F1: 0.8095238095238095, Precision: 0.8095238095238095\n","K-Fold:  4 Epoch:  87 Train loss:  0.1053248330884214\n","K-Fold: 4, Epoch: 87, Accuracy: 0.780952380952381, Recall: 0.780952380952381, F1: 0.780952380952381, Precision: 0.780952380952381\n","K-Fold:  4 Epoch:  88 Train loss:  0.08046126641732242\n","K-Fold: 4, Epoch: 88, Accuracy: 0.7238095238095238, Recall: 0.7238095238095238, F1: 0.7238095238095238, Precision: 0.7238095238095238\n","K-Fold:  4 Epoch:  89 Train loss:  0.01696623833517411\n","K-Fold: 4, Epoch: 89, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  4 Epoch:  90 Train loss:  0.026659893312691047\n","K-Fold: 4, Epoch: 90, Accuracy: 0.6857142857142857, Recall: 0.6857142857142857, F1: 0.6857142857142857, Precision: 0.6857142857142857\n","K-Fold:  4 Epoch:  91 Train loss:  0.01270055553842602\n","K-Fold: 4, Epoch: 91, Accuracy: 0.8285714285714286, Recall: 0.8285714285714286, F1: 0.8285714285714286, Precision: 0.8285714285714286\n","K-Fold:  4 Epoch:  92 Train loss:  0.005157874157053551\n","K-Fold: 4, Epoch: 92, Accuracy: 0.8476190476190476, Recall: 0.8476190476190476, F1: 0.8476190476190476, Precision: 0.8476190476190476\n","K-Fold:  4 Epoch:  93 Train loss:  0.005202669858200741\n","K-Fold: 4, Epoch: 93, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  4 Epoch:  94 Train loss:  0.0015453026738084321\n","K-Fold: 4, Epoch: 94, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  4 Epoch:  95 Train loss:  0.01258078143291641\n","K-Fold: 4, Epoch: 95, Accuracy: 0.8761904761904762, Recall: 0.8761904761904762, F1: 0.8761904761904762, Precision: 0.8761904761904762\n","K-Fold:  4 Epoch:  96 Train loss:  0.014641928428318352\n","K-Fold: 4, Epoch: 96, Accuracy: 0.819047619047619, Recall: 0.819047619047619, F1: 0.819047619047619, Precision: 0.819047619047619\n","K-Fold:  4 Epoch:  97 Train loss:  0.007468392685820747\n","K-Fold: 4, Epoch: 97, Accuracy: 0.8571428571428571, Recall: 0.8571428571428571, F1: 0.8571428571428571, Precision: 0.8571428571428571\n","K-Fold:  4 Epoch:  98 Train loss:  0.0061455807631968385\n","K-Fold: 4, Epoch: 98, Accuracy: 0.8666666666666667, Recall: 0.8666666666666667, F1: 0.8666666666666667, Precision: 0.8666666666666667\n","K-Fold:  4 Epoch:  99 Train loss:  0.0011961440801055037\n","K-Fold: 4, Epoch: 99, Accuracy: 0.8857142857142857, Recall: 0.8857142857142857, F1: 0.8857142857142857, Precision: 0.8857142857142857\n"]}],"source":["for idx, (train_idx, test_idx) in enumerate(skf.split(mfccs, labels)):\n","\n","    mfcc_train, mfcc_test = mfccs[train_idx], mfccs[test_idx]\n","    train_labels, test_labels = labels[train_idx], labels[test_idx]\n","\n","    train_labels = [EMO_CLASSES[label] for label in train_labels]\n","    test_labels = [EMO_CLASSES[label] for label in test_labels]\n","\n","\n","    train_dataset = EmoDataset(mfcc_train, train_labels)\n","    test_dataset = EmoDataset(mfcc_test, test_labels)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    model = Dual()\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    best_acc = 0\n","    best_f1 = 0\n","    best_recall = 0\n","    best_precision = 0\n","\n","    acc = []\n","    f1 = []\n","    recall = []\n","    precision = []\n","    best_labels_preds = []\n","\n","    for epoch in range(epochs):\n","        tt_loss = 0\n","        model.train()\n","        for mfcc, label in train_loader:\n","\n","            mfcc = mfcc.unsqueeze(1).to(device)\n","            label = label.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(mfcc)\n","\n","            loss = loss_fn(output, label)\n","            loss.backward()\n","            optimizer.step()\n","            tt_loss += loss.item()\n","\n","        print(\"K-Fold: \", idx, \"Epoch: \", epoch, \"Train loss: \", tt_loss / len(train_loader))\n","\n","        model.eval()\n","\n","        with torch.no_grad():\n","            all_labels = []\n","            all_preds = []\n","\n","            for mfcc, label in test_loader:\n","\n","                mfcc = mfcc.unsqueeze(1).to(device)\n","                label = label.to(device)\n","\n","                output = model(mfcc)\n","                _, predicted = torch.max(output, 1)\n","\n","                all_labels.extend(label.cpu().numpy())\n","                all_preds.extend(predicted.cpu().numpy())\n","\n","\n","            validation_acc = accuracy_score(all_labels, all_preds)\n","            recall_val = recall_score(all_labels, all_preds, average=avarage)\n","            f1_val = f1_score(all_labels, all_preds, average=avarage)\n","            precision_val = precision_score(all_labels, all_preds, average=avarage)\n","\n","            acc.append(validation_acc)\n","            f1.append(f1_val)\n","            recall.append(recall_val)\n","            precision.append(precision_val)\n","\n","\n","            if validation_acc > best_acc:\n","                best_acc = validation_acc\n","                best_recall = recall_val\n","                best_f1 = f1_val\n","                best_precision = precision_val\n","                best_labels_preds = [all_labels, all_preds]\n","\n","                # torch.save(model.state_dict(), f\"best_model_{idx}.pth\")\n","\n","            print(f\"K-Fold: {idx}, Epoch: {epoch}, Accuracy: {validation_acc}, Recall: {recall_val}, F1: {f1_val}, Precision: {precision_val}\")\n","\n","    data_kfold.append([acc, f1, recall, precision])\n","\n","    acc_kfold.append(best_acc)\n","    f1_kfold.append(best_f1)\n","    recall_kfold.append(best_recall)\n","    precision_kfold.append(best_precision)\n","    labels_preds_kfold.append(best_labels_preds)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"iE1IjORJatCe","outputId":"49596992-38dc-423c-b859-1c71547ae8a2"},"outputs":[{"data":{"text/plain":["5"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["len(data_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"AyFILjk9atCf","outputId":"c27fcd6a-9025-4e39-86c2-7c786250dac9"},"outputs":[{"data":{"text/plain":["5"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["len(labels_preds_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjIDNb4tatCf"},"outputs":[],"source":["def print_classification_report(labels_preds_kfold):\n","    for idx, data in enumerate(labels_preds_kfold):\n","        print(f\"K-Fold: {idx}\")\n","        print(classification_report(data[0], data[1], target_names=EMO_CLASSES.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPV65lfIatCf"},"outputs":[],"source":["def plot_metrics(data_kfold):\n","    num_folds = len(data_kfold)\n","    plt.figure(figsize=(20, 5))\n","\n","    for idx, data in enumerate(data_kfold):\n","        acc, f1, recall, precision = data\n","        plt.subplot(1, num_folds, idx + 1)\n","        plt.plot(acc, label='Accuracy')\n","        plt.plot(recall, label='Recall')\n","        plt.plot(f1, label='F1')\n","        plt.plot(precision, label='Precision')\n","        plt.title(f\"K-Fold {idx + 1}\")\n","        plt.xlabel('Epochs')\n","        plt.ylabel('Metrics')\n","        plt.ylim(0.0, 1.0)  # Set y-axis range from 0.0 to 1.0\n","        plt.legend()\n","        plt.grid(True)\n","\n","    plt.tight_layout()  # Adjust subplots to fit into figure area\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"ovpvOeOtatCf","outputId":"f323e381-a1a5-4050-819d-c4732d923cf6"},"outputs":[{"data":{"text/plain":["[0.8113207547169812,\n"," 0.8207547169811321,\n"," 0.8962264150943396,\n"," 0.9142857142857143,\n"," 0.8952380952380953]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["acc_kfold"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"KP86r3RzatCf","outputId":"23280adf-8654-4b3a-9962-c0b7dec1058c"},"outputs":[{"data":{"text/plain":["(0.8675651392632526,\n"," 0.8675651392632526,\n"," 0.8675651392632526,\n"," 0.8675651392632526)"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["np.mean(acc_kfold),np.mean(f1_kfold),np.mean(precision_kfold),np.mean(f1_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1724873878711,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"vP95sMFKatCf","outputId":"2f726521-0851-41a0-dd84-38b81e425d07"},"outputs":[{"name":"stdout","output_type":"stream","text":["K-Fold: 0\n","              precision    recall  f1-score   support\n","\n","     sadness       0.83      0.73      0.78        26\n","     neutral       0.84      0.95      0.89        22\n","   happiness       0.71      0.80      0.75        25\n","        fear       0.87      0.81      0.84        16\n","       angry       0.87      0.76      0.81        17\n","\n","    accuracy                           0.81       106\n","   macro avg       0.82      0.81      0.82       106\n","weighted avg       0.82      0.81      0.81       106\n","\n","K-Fold: 1\n","              precision    recall  f1-score   support\n","\n","     sadness       0.86      0.93      0.89        27\n","     neutral       0.84      0.95      0.89        22\n","   happiness       0.77      0.68      0.72        25\n","        fear       0.87      0.87      0.87        15\n","       angry       0.73      0.65      0.69        17\n","\n","    accuracy                           0.82       106\n","   macro avg       0.81      0.81      0.81       106\n","weighted avg       0.82      0.82      0.82       106\n","\n","K-Fold: 2\n","              precision    recall  f1-score   support\n","\n","     sadness       0.90      1.00      0.95        27\n","     neutral       1.00      0.86      0.93        22\n","   happiness       0.85      0.92      0.88        24\n","        fear       0.86      0.75      0.80        16\n","       angry       0.88      0.88      0.88        17\n","\n","    accuracy                           0.90       106\n","   macro avg       0.90      0.88      0.89       106\n","weighted avg       0.90      0.90      0.90       106\n","\n","K-Fold: 3\n","              precision    recall  f1-score   support\n","\n","     sadness       0.96      0.93      0.94        27\n","     neutral       0.91      0.91      0.91        22\n","   happiness       0.88      0.96      0.92        24\n","        fear       0.93      0.81      0.87        16\n","       angry       0.88      0.94      0.91        16\n","\n","    accuracy                           0.91       105\n","   macro avg       0.91      0.91      0.91       105\n","weighted avg       0.92      0.91      0.91       105\n","\n","K-Fold: 4\n","              precision    recall  f1-score   support\n","\n","     sadness       0.93      0.93      0.93        27\n","     neutral       1.00      0.95      0.98        22\n","   happiness       0.77      0.96      0.85        24\n","        fear       1.00      0.69      0.81        16\n","       angry       0.88      0.88      0.88        16\n","\n","    accuracy                           0.90       105\n","   macro avg       0.91      0.88      0.89       105\n","weighted avg       0.91      0.90      0.89       105\n","\n"]}],"source":["print_classification_report(labels_preds_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gf6tl6lsatCf"},"outputs":[],"source":["# average classification report\n","def average_classification_report(labels_preds_kfold):\n","    all_labels = []\n","    all_preds = []\n","    for data in labels_preds_kfold:\n","        all_labels.extend(data[0])\n","        all_preds.extend(data[1])\n","    print(classification_report(all_labels, all_preds, target_names=EMO_CLASSES.keys()))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1724873878712,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"nZfhCGXvatCf","outputId":"803ab22c-54bf-4880-9cf8-75a9d8ce7217"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","     sadness       0.90      0.90      0.90       134\n","     neutral       0.91      0.93      0.92       110\n","   happiness       0.80      0.86      0.83       122\n","        fear       0.90      0.78      0.84        79\n","       angry       0.85      0.82      0.83        83\n","\n","    accuracy                           0.87       528\n","   macro avg       0.87      0.86      0.86       528\n","weighted avg       0.87      0.87      0.87       528\n","\n"]}],"source":["average_classification_report(labels_preds_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-gn7XU9jatCg"},"outputs":[],"source":["# plot confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","\n","def plot_confusion_matrix(labels_preds_kfold):\n","    all_labels = []\n","    all_preds = []\n","    for data in labels_preds_kfold:\n","        all_labels.extend(data[0])\n","        all_preds.extend(data[1])\n","\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMO_CLASSES.keys(), yticklabels=EMO_CLASSES.keys())\n","    plt.xlabel('Predicted labels')\n","    plt.ylabel('True labels')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1724873879739,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"MGDIQtLpatCg","outputId":"0c719a5f-d32a-47db-c7d8-87dc23250dc4"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plot_confusion_matrix(labels_preds_kfold)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhNr4WMWatCg"},"outputs":[],"source":["def compute_ua_wa(labels_preds_kfold):\n","    true_labels = np.array([])\n","    predicted_labels = np.array([])\n","\n","    # Gộp tất cả nhãn thực và nhãn dự đoán từ các fold\n","    for data in labels_preds_kfold:\n","        true_labels = np.append(true_labels, data[0])\n","        predicted_labels = np.append(predicted_labels, data[1])\n","\n","    # Tính UA (Unweighted Accuracy)\n","    ua = accuracy_score(true_labels, predicted_labels)\n","\n","    # Tính WA (Weighted Accuracy)\n","    wa = []\n","    for label in np.unique(true_labels):\n","        i_true = true_labels[true_labels == label]\n","        i_predicted = predicted_labels[true_labels == label]\n","\n","        wa.append(np.sum(i_true == i_predicted) / len(i_true))\n","    wa = np.mean(wa)\n","    return ua, wa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1724873879739,"user":{"displayName":"Nguyễn Đức Quang Anh","userId":"05768917261476937201"},"user_tz":-420},"id":"gMqdLLOFatCg","outputId":"78c38944-c00d-40b1-f60c-4f27fa772b35"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unweighted Accuracy: 0.8674242424242424, Weighted Accuracy: 0.8590001549241049\n"]}],"source":["ua, wa = compute_ua_wa(labels_preds_kfold)\n","print(f\"Unweighted Accuracy: {ua}, Weighted Accuracy: {wa}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-_TyZc8bQhR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
